{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### DS-207: Kaggle Competition | EEG Seizure Prediction Competition\n",
    "\n",
    ">Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective. Even after surgical removal of epilepsy, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring.\n",
    "\n",
    ">Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for electrical brain activity (EEG) based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.\\n\",\n",
    "\n",
    "From the competition [homepage](https://www.kaggle.com/c/melbourne-university-seizure-prediction).\n",
    "\n",
    "### Goal for this Notebook:\n",
    "* Use L1-regularized LR to select features\n",
    "* Parameter tuning using Bayesian optimization\n",
    "* Learn how to use xgboost\n",
    "* Use stacking to ensemble models\n",
    "\n",
    "#### Required Libraries:\n",
    "* [NumPy](http://www.numpy.org/)\n",
    "* [IPython](http://ipython.org/)\n",
    "* [Pandas](http://pandas.pydata.org/)\n",
    "* [SciKit-Learn](http://scikit-learn.org/stable/)\n",
    "* [Matplotlib](http://matplotlib.org/)\n",
    "* [Xgboost](http://xgboost.readthedocs.io/en/latest/build.html)\n",
    "* [bayes_opt]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Datawarehouse():\n",
    "    def __init__(self):\n",
    "        # Initialize Datawarehouse.\n",
    "        self.TRAIN_FILE = '../input/train.csv'\n",
    "        self.TEST_FILE = '../input/test.csv'\n",
    "        self.GROUP_FILE = '../input/group.csv'\n",
    "        self.DATA_ID = 'File'\n",
    "        self.DATA_OUT_NAME = 'Class'\n",
    "    \n",
    "    def read_data(self):\n",
    "        # Read in training and testing data for further processing\n",
    "        self.train_in = pd.read_csv(self.TRAIN_FILE)\n",
    "        self.test_in = pd.read_csv(self.TEST_FILE)\n",
    "        self.group = pd.read_csv(self.GROUP_FILE)\n",
    "        \n",
    "        # get response variable and id\n",
    "        self.col_name = self.train_in.columns\n",
    "        self.train_out = self.train_in[self.DATA_OUT_NAME]\n",
    "        self.train_id = self.train_in[self.DATA_ID]\n",
    "\n",
    "        # remove unnecessary information\n",
    "        del self.train_in[self.DATA_ID]\n",
    "        del self.train_in[self.DATA_OUT_NAME]\n",
    "\n",
    "        # get test id\n",
    "        self.test_id = self.test_in[self.DATA_ID]\n",
    "        \n",
    "        # remove unnecessary information from test as well\n",
    "        del self.test_in[self.DATA_ID]\n",
    "\n",
    "        # get group\n",
    "        del self.group[self.DATA_ID]\n",
    "\n",
    "        self.train_in = self.train_in.as_matrix()\n",
    "        self.train_out = self.train_out.as_matrix()\n",
    "        self.test_in = self.test_in.as_matrix()\n",
    "        self.group = self.group.as_matrix()\n",
    "    \n",
    "    def select_features(self, vec):\n",
    "        \"\"\" Select features given a boolean vector\n",
    "\n",
    "        Arguments:\n",
    "            vec (ndarray): a boolean vector to indicate what features to keep\n",
    "        \"\"\"\n",
    "        self.train_in = self.train_in[:,vec]\n",
    "        self.test_in = self.test_in[:,vec]\n",
    "    \n",
    "    def gen_submission(self, ypred, filename):\n",
    "        \"\"\" Generate submission files based on given prediction results\n",
    "\n",
    "        Arguments:\n",
    "            ypred (ndarray): a numeric vector storing probability\n",
    "        \"\"\"\n",
    "        dataset = list(zip(self.test_id, ypred))\n",
    "\n",
    "        # transform list to data frame\n",
    "        df_pred = pd.DataFrame(dataset, columns = [self.DATA_ID, self.DATA_OUT_NAME], index=None)\n",
    "        df_pred.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>SigMean_C1</th>\n",
       "      <th>SigMean_C2</th>\n",
       "      <th>SigMean_C3</th>\n",
       "      <th>SigMean_C4</th>\n",
       "      <th>SigMean_C5</th>\n",
       "      <th>SigMean_C6</th>\n",
       "      <th>SigMean_C7</th>\n",
       "      <th>SigMean_C8</th>\n",
       "      <th>SigMean_C9</th>\n",
       "      <th>...</th>\n",
       "      <th>WaveletR8_C8</th>\n",
       "      <th>WaveletR8_C9</th>\n",
       "      <th>WaveletR8_C10</th>\n",
       "      <th>WaveletR8_C11</th>\n",
       "      <th>WaveletR8_C12</th>\n",
       "      <th>WaveletR8_C13</th>\n",
       "      <th>WaveletR8_C14</th>\n",
       "      <th>WaveletR8_C15</th>\n",
       "      <th>WaveletR8_C16</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7_0.mat</td>\n",
       "      <td>-0.288530</td>\n",
       "      <td>0.324564</td>\n",
       "      <td>-0.002718</td>\n",
       "      <td>0.050880</td>\n",
       "      <td>0.146694</td>\n",
       "      <td>-0.736724</td>\n",
       "      <td>-0.265787</td>\n",
       "      <td>-0.800872</td>\n",
       "      <td>-0.454978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394613</td>\n",
       "      <td>1.075273</td>\n",
       "      <td>-0.866328</td>\n",
       "      <td>-0.078471</td>\n",
       "      <td>-0.640318</td>\n",
       "      <td>-0.210430</td>\n",
       "      <td>-0.824698</td>\n",
       "      <td>0.523578</td>\n",
       "      <td>-0.091089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_8_0.mat</td>\n",
       "      <td>-0.048981</td>\n",
       "      <td>-0.093086</td>\n",
       "      <td>-0.123211</td>\n",
       "      <td>0.576776</td>\n",
       "      <td>-0.059393</td>\n",
       "      <td>-0.626494</td>\n",
       "      <td>-0.288666</td>\n",
       "      <td>-1.123369</td>\n",
       "      <td>-1.058181</td>\n",
       "      <td>...</td>\n",
       "      <td>3.960875</td>\n",
       "      <td>3.582588</td>\n",
       "      <td>-0.785167</td>\n",
       "      <td>1.371965</td>\n",
       "      <td>-0.095059</td>\n",
       "      <td>-0.210288</td>\n",
       "      <td>-0.517351</td>\n",
       "      <td>3.076091</td>\n",
       "      <td>1.487048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_9_0.mat</td>\n",
       "      <td>-0.168991</td>\n",
       "      <td>0.137907</td>\n",
       "      <td>0.106642</td>\n",
       "      <td>-0.239069</td>\n",
       "      <td>-0.695586</td>\n",
       "      <td>-1.335152</td>\n",
       "      <td>-0.421489</td>\n",
       "      <td>-1.152721</td>\n",
       "      <td>-0.869702</td>\n",
       "      <td>...</td>\n",
       "      <td>4.068216</td>\n",
       "      <td>3.496801</td>\n",
       "      <td>-0.770978</td>\n",
       "      <td>1.984995</td>\n",
       "      <td>0.122791</td>\n",
       "      <td>-0.209740</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>3.771308</td>\n",
       "      <td>2.205124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_10_0.mat</td>\n",
       "      <td>-0.199351</td>\n",
       "      <td>-0.573721</td>\n",
       "      <td>-0.527654</td>\n",
       "      <td>0.060454</td>\n",
       "      <td>-0.342314</td>\n",
       "      <td>-1.070089</td>\n",
       "      <td>-0.899636</td>\n",
       "      <td>-1.362659</td>\n",
       "      <td>-1.266849</td>\n",
       "      <td>...</td>\n",
       "      <td>3.214353</td>\n",
       "      <td>3.345645</td>\n",
       "      <td>-0.776387</td>\n",
       "      <td>1.197794</td>\n",
       "      <td>-0.173821</td>\n",
       "      <td>-0.210032</td>\n",
       "      <td>-0.429824</td>\n",
       "      <td>2.981368</td>\n",
       "      <td>1.241364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_11_0.mat</td>\n",
       "      <td>0.017845</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>-0.022627</td>\n",
       "      <td>0.094499</td>\n",
       "      <td>-0.046953</td>\n",
       "      <td>-0.936635</td>\n",
       "      <td>-0.546479</td>\n",
       "      <td>-0.991634</td>\n",
       "      <td>-0.663843</td>\n",
       "      <td>...</td>\n",
       "      <td>4.601992</td>\n",
       "      <td>3.607907</td>\n",
       "      <td>-0.779511</td>\n",
       "      <td>1.731818</td>\n",
       "      <td>-0.051429</td>\n",
       "      <td>-0.210167</td>\n",
       "      <td>-0.369933</td>\n",
       "      <td>4.052383</td>\n",
       "      <td>2.052193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1_12_0.mat</td>\n",
       "      <td>0.014489</td>\n",
       "      <td>0.130260</td>\n",
       "      <td>0.082293</td>\n",
       "      <td>0.071109</td>\n",
       "      <td>-0.102388</td>\n",
       "      <td>-1.003618</td>\n",
       "      <td>-0.422200</td>\n",
       "      <td>-1.093781</td>\n",
       "      <td>-0.668133</td>\n",
       "      <td>...</td>\n",
       "      <td>5.240694</td>\n",
       "      <td>4.073852</td>\n",
       "      <td>-0.767021</td>\n",
       "      <td>1.792692</td>\n",
       "      <td>-0.015426</td>\n",
       "      <td>-0.210365</td>\n",
       "      <td>-0.517251</td>\n",
       "      <td>3.994455</td>\n",
       "      <td>2.011985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1_13_0.mat</td>\n",
       "      <td>-0.655157</td>\n",
       "      <td>0.457431</td>\n",
       "      <td>0.140928</td>\n",
       "      <td>0.072871</td>\n",
       "      <td>-0.366381</td>\n",
       "      <td>-1.075830</td>\n",
       "      <td>-0.403581</td>\n",
       "      <td>-1.694735</td>\n",
       "      <td>-0.391248</td>\n",
       "      <td>...</td>\n",
       "      <td>5.449587</td>\n",
       "      <td>2.865873</td>\n",
       "      <td>-0.748247</td>\n",
       "      <td>1.190386</td>\n",
       "      <td>-0.160518</td>\n",
       "      <td>-0.209625</td>\n",
       "      <td>0.293477</td>\n",
       "      <td>2.675517</td>\n",
       "      <td>2.167857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1_14_0.mat</td>\n",
       "      <td>0.576938</td>\n",
       "      <td>-0.021378</td>\n",
       "      <td>1.319599</td>\n",
       "      <td>-0.323885</td>\n",
       "      <td>-1.541729</td>\n",
       "      <td>-0.644021</td>\n",
       "      <td>-0.693023</td>\n",
       "      <td>-3.264855</td>\n",
       "      <td>1.226745</td>\n",
       "      <td>...</td>\n",
       "      <td>5.731851</td>\n",
       "      <td>4.206076</td>\n",
       "      <td>-0.651534</td>\n",
       "      <td>2.545810</td>\n",
       "      <td>0.387698</td>\n",
       "      <td>-0.209287</td>\n",
       "      <td>0.332244</td>\n",
       "      <td>4.419004</td>\n",
       "      <td>2.935790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1_15_0.mat</td>\n",
       "      <td>0.508465</td>\n",
       "      <td>-0.365782</td>\n",
       "      <td>-0.653625</td>\n",
       "      <td>-0.290081</td>\n",
       "      <td>0.490278</td>\n",
       "      <td>0.147102</td>\n",
       "      <td>-0.857894</td>\n",
       "      <td>-0.593983</td>\n",
       "      <td>-0.182086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.408528</td>\n",
       "      <td>0.264590</td>\n",
       "      <td>-0.894032</td>\n",
       "      <td>-0.706963</td>\n",
       "      <td>-0.830039</td>\n",
       "      <td>-0.210508</td>\n",
       "      <td>-0.961791</td>\n",
       "      <td>-0.352436</td>\n",
       "      <td>-0.911083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1_16_0.mat</td>\n",
       "      <td>-0.017030</td>\n",
       "      <td>0.316921</td>\n",
       "      <td>0.026681</td>\n",
       "      <td>0.078928</td>\n",
       "      <td>-0.083321</td>\n",
       "      <td>-0.622562</td>\n",
       "      <td>-0.509687</td>\n",
       "      <td>-0.653605</td>\n",
       "      <td>-0.235385</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.645358</td>\n",
       "      <td>-0.752254</td>\n",
       "      <td>-0.931867</td>\n",
       "      <td>-1.095465</td>\n",
       "      <td>-0.994557</td>\n",
       "      <td>-0.210540</td>\n",
       "      <td>-1.031693</td>\n",
       "      <td>-1.033472</td>\n",
       "      <td>-1.365559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1_17_0.mat</td>\n",
       "      <td>-0.107355</td>\n",
       "      <td>-0.162953</td>\n",
       "      <td>-0.996098</td>\n",
       "      <td>2.808183</td>\n",
       "      <td>-0.174293</td>\n",
       "      <td>-1.528328</td>\n",
       "      <td>-2.178322</td>\n",
       "      <td>-0.614406</td>\n",
       "      <td>0.757732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248843</td>\n",
       "      <td>-0.743139</td>\n",
       "      <td>-0.915971</td>\n",
       "      <td>-0.976023</td>\n",
       "      <td>-0.937788</td>\n",
       "      <td>-0.210552</td>\n",
       "      <td>-1.006010</td>\n",
       "      <td>-0.932202</td>\n",
       "      <td>-1.320866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1_18_0.mat</td>\n",
       "      <td>2.468389</td>\n",
       "      <td>0.736495</td>\n",
       "      <td>-0.318129</td>\n",
       "      <td>2.140369</td>\n",
       "      <td>-1.028426</td>\n",
       "      <td>-1.562106</td>\n",
       "      <td>-1.514582</td>\n",
       "      <td>-1.777203</td>\n",
       "      <td>-0.097207</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216453</td>\n",
       "      <td>-0.389211</td>\n",
       "      <td>-0.854222</td>\n",
       "      <td>-0.603501</td>\n",
       "      <td>-0.828734</td>\n",
       "      <td>-0.210379</td>\n",
       "      <td>-0.799011</td>\n",
       "      <td>-0.341136</td>\n",
       "      <td>-0.910694</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1_19_0.mat</td>\n",
       "      <td>0.016263</td>\n",
       "      <td>0.226342</td>\n",
       "      <td>0.117856</td>\n",
       "      <td>0.140571</td>\n",
       "      <td>-0.037668</td>\n",
       "      <td>-0.728453</td>\n",
       "      <td>-0.520898</td>\n",
       "      <td>-0.852833</td>\n",
       "      <td>-0.396602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339248</td>\n",
       "      <td>0.314408</td>\n",
       "      <td>-0.836682</td>\n",
       "      <td>-0.219681</td>\n",
       "      <td>-0.651738</td>\n",
       "      <td>-0.210252</td>\n",
       "      <td>-0.649826</td>\n",
       "      <td>0.389490</td>\n",
       "      <td>-0.092823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1_20_0.mat</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.240421</td>\n",
       "      <td>0.070596</td>\n",
       "      <td>0.148317</td>\n",
       "      <td>-0.090425</td>\n",
       "      <td>-0.735969</td>\n",
       "      <td>-0.485500</td>\n",
       "      <td>-0.761253</td>\n",
       "      <td>-0.357012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248185</td>\n",
       "      <td>0.373849</td>\n",
       "      <td>-0.856493</td>\n",
       "      <td>-0.219434</td>\n",
       "      <td>-0.694672</td>\n",
       "      <td>-0.210431</td>\n",
       "      <td>-0.812970</td>\n",
       "      <td>0.357022</td>\n",
       "      <td>-0.515895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1_21_0.mat</td>\n",
       "      <td>0.086422</td>\n",
       "      <td>0.327529</td>\n",
       "      <td>-0.013377</td>\n",
       "      <td>-0.019369</td>\n",
       "      <td>-0.114642</td>\n",
       "      <td>-0.624669</td>\n",
       "      <td>-0.486112</td>\n",
       "      <td>-0.737054</td>\n",
       "      <td>-0.382490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176888</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>-0.836551</td>\n",
       "      <td>-0.381501</td>\n",
       "      <td>-0.700757</td>\n",
       "      <td>-0.210453</td>\n",
       "      <td>-0.860743</td>\n",
       "      <td>0.273623</td>\n",
       "      <td>-0.483924</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1_22_0.mat</td>\n",
       "      <td>0.014130</td>\n",
       "      <td>0.231487</td>\n",
       "      <td>0.116156</td>\n",
       "      <td>0.041383</td>\n",
       "      <td>-0.093172</td>\n",
       "      <td>-0.889774</td>\n",
       "      <td>-0.514569</td>\n",
       "      <td>-1.004461</td>\n",
       "      <td>-0.467471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682073</td>\n",
       "      <td>1.264365</td>\n",
       "      <td>-0.824499</td>\n",
       "      <td>0.301450</td>\n",
       "      <td>-0.554697</td>\n",
       "      <td>-0.210486</td>\n",
       "      <td>-0.858859</td>\n",
       "      <td>1.086474</td>\n",
       "      <td>-0.419907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1_23_0.mat</td>\n",
       "      <td>0.016582</td>\n",
       "      <td>0.251332</td>\n",
       "      <td>0.109356</td>\n",
       "      <td>0.120858</td>\n",
       "      <td>-0.052773</td>\n",
       "      <td>-0.742272</td>\n",
       "      <td>-0.494545</td>\n",
       "      <td>-0.866886</td>\n",
       "      <td>-0.374134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293572</td>\n",
       "      <td>0.761325</td>\n",
       "      <td>-0.845750</td>\n",
       "      <td>0.022369</td>\n",
       "      <td>-0.624567</td>\n",
       "      <td>-0.210493</td>\n",
       "      <td>-0.892961</td>\n",
       "      <td>0.592680</td>\n",
       "      <td>-0.672328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1_24_0.mat</td>\n",
       "      <td>-5.874305</td>\n",
       "      <td>7.124888</td>\n",
       "      <td>-4.263323</td>\n",
       "      <td>-0.992420</td>\n",
       "      <td>-2.913278</td>\n",
       "      <td>1.678971</td>\n",
       "      <td>4.676792</td>\n",
       "      <td>-5.520978</td>\n",
       "      <td>-3.742057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389553</td>\n",
       "      <td>0.127452</td>\n",
       "      <td>-0.892874</td>\n",
       "      <td>-0.529728</td>\n",
       "      <td>-0.675885</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>0.197999</td>\n",
       "      <td>0.054220</td>\n",
       "      <td>-0.017420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1_25_0.mat</td>\n",
       "      <td>2.389926</td>\n",
       "      <td>-3.164038</td>\n",
       "      <td>0.647671</td>\n",
       "      <td>-3.416691</td>\n",
       "      <td>-1.308395</td>\n",
       "      <td>-1.356191</td>\n",
       "      <td>-3.886885</td>\n",
       "      <td>-2.056498</td>\n",
       "      <td>3.684275</td>\n",
       "      <td>...</td>\n",
       "      <td>5.688695</td>\n",
       "      <td>1.067532</td>\n",
       "      <td>-0.638198</td>\n",
       "      <td>2.786467</td>\n",
       "      <td>0.128534</td>\n",
       "      <td>-0.209757</td>\n",
       "      <td>0.127087</td>\n",
       "      <td>4.108325</td>\n",
       "      <td>2.613091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1_26_0.mat</td>\n",
       "      <td>0.122764</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.075340</td>\n",
       "      <td>0.180366</td>\n",
       "      <td>-0.290268</td>\n",
       "      <td>-1.185950</td>\n",
       "      <td>-0.712845</td>\n",
       "      <td>-1.475375</td>\n",
       "      <td>-0.622130</td>\n",
       "      <td>...</td>\n",
       "      <td>5.094197</td>\n",
       "      <td>3.242722</td>\n",
       "      <td>-0.554116</td>\n",
       "      <td>3.723095</td>\n",
       "      <td>0.684206</td>\n",
       "      <td>-0.210126</td>\n",
       "      <td>-0.361573</td>\n",
       "      <td>5.107177</td>\n",
       "      <td>2.191459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1_27_0.mat</td>\n",
       "      <td>-0.049447</td>\n",
       "      <td>0.354053</td>\n",
       "      <td>0.457940</td>\n",
       "      <td>0.352845</td>\n",
       "      <td>-0.045770</td>\n",
       "      <td>-1.091065</td>\n",
       "      <td>-0.909598</td>\n",
       "      <td>-0.971723</td>\n",
       "      <td>-0.615520</td>\n",
       "      <td>...</td>\n",
       "      <td>3.860770</td>\n",
       "      <td>2.812556</td>\n",
       "      <td>-0.704434</td>\n",
       "      <td>2.430364</td>\n",
       "      <td>0.364384</td>\n",
       "      <td>-0.209966</td>\n",
       "      <td>-0.272638</td>\n",
       "      <td>4.633073</td>\n",
       "      <td>2.003624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1_28_0.mat</td>\n",
       "      <td>-0.978077</td>\n",
       "      <td>-4.088753</td>\n",
       "      <td>-1.917240</td>\n",
       "      <td>-5.834190</td>\n",
       "      <td>0.823131</td>\n",
       "      <td>-3.208397</td>\n",
       "      <td>2.904948</td>\n",
       "      <td>-2.675808</td>\n",
       "      <td>-2.962324</td>\n",
       "      <td>...</td>\n",
       "      <td>4.308685</td>\n",
       "      <td>2.604854</td>\n",
       "      <td>-0.683105</td>\n",
       "      <td>2.233156</td>\n",
       "      <td>0.305950</td>\n",
       "      <td>-0.209304</td>\n",
       "      <td>-0.140386</td>\n",
       "      <td>4.158798</td>\n",
       "      <td>2.974046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1_29_0.mat</td>\n",
       "      <td>0.807601</td>\n",
       "      <td>-1.820035</td>\n",
       "      <td>0.401598</td>\n",
       "      <td>0.706551</td>\n",
       "      <td>-0.017233</td>\n",
       "      <td>-0.653581</td>\n",
       "      <td>-0.482750</td>\n",
       "      <td>-1.127154</td>\n",
       "      <td>-0.552759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555284</td>\n",
       "      <td>1.417922</td>\n",
       "      <td>-0.811425</td>\n",
       "      <td>-0.516092</td>\n",
       "      <td>-0.663596</td>\n",
       "      <td>-0.210252</td>\n",
       "      <td>-0.745527</td>\n",
       "      <td>0.149602</td>\n",
       "      <td>-0.260921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1_30_0.mat</td>\n",
       "      <td>2.150922</td>\n",
       "      <td>-1.918457</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>1.746149</td>\n",
       "      <td>-1.079286</td>\n",
       "      <td>-1.321718</td>\n",
       "      <td>-0.367232</td>\n",
       "      <td>-2.488519</td>\n",
       "      <td>-0.347563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578934</td>\n",
       "      <td>0.796345</td>\n",
       "      <td>-0.819136</td>\n",
       "      <td>0.081394</td>\n",
       "      <td>-0.585357</td>\n",
       "      <td>-0.210318</td>\n",
       "      <td>-0.654840</td>\n",
       "      <td>0.823207</td>\n",
       "      <td>-0.201391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1_31_0.mat</td>\n",
       "      <td>-0.059716</td>\n",
       "      <td>-0.413812</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.167308</td>\n",
       "      <td>-0.307473</td>\n",
       "      <td>-1.267701</td>\n",
       "      <td>-0.479681</td>\n",
       "      <td>-1.133853</td>\n",
       "      <td>-0.908891</td>\n",
       "      <td>...</td>\n",
       "      <td>1.819994</td>\n",
       "      <td>2.680907</td>\n",
       "      <td>-0.730192</td>\n",
       "      <td>1.843782</td>\n",
       "      <td>-0.099635</td>\n",
       "      <td>-0.210265</td>\n",
       "      <td>-0.586212</td>\n",
       "      <td>3.195324</td>\n",
       "      <td>0.761748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1_32_0.mat</td>\n",
       "      <td>-0.238290</td>\n",
       "      <td>0.199344</td>\n",
       "      <td>0.008342</td>\n",
       "      <td>0.479131</td>\n",
       "      <td>-1.533048</td>\n",
       "      <td>-1.264341</td>\n",
       "      <td>0.169659</td>\n",
       "      <td>-1.351542</td>\n",
       "      <td>-0.741736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564346</td>\n",
       "      <td>0.866977</td>\n",
       "      <td>-0.803806</td>\n",
       "      <td>0.315601</td>\n",
       "      <td>-0.495856</td>\n",
       "      <td>-0.210234</td>\n",
       "      <td>-0.672642</td>\n",
       "      <td>1.242084</td>\n",
       "      <td>-0.114290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1_33_0.mat</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>0.248512</td>\n",
       "      <td>0.180592</td>\n",
       "      <td>0.094865</td>\n",
       "      <td>-0.019913</td>\n",
       "      <td>-0.762517</td>\n",
       "      <td>-0.533509</td>\n",
       "      <td>-0.889742</td>\n",
       "      <td>-0.404175</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.038268</td>\n",
       "      <td>0.297366</td>\n",
       "      <td>-0.877518</td>\n",
       "      <td>-0.724054</td>\n",
       "      <td>-0.852111</td>\n",
       "      <td>-0.210530</td>\n",
       "      <td>-0.969573</td>\n",
       "      <td>-0.630685</td>\n",
       "      <td>-1.106383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1_34_0.mat</td>\n",
       "      <td>0.010845</td>\n",
       "      <td>0.257040</td>\n",
       "      <td>0.077783</td>\n",
       "      <td>0.115970</td>\n",
       "      <td>-0.022243</td>\n",
       "      <td>-0.745458</td>\n",
       "      <td>-0.510756</td>\n",
       "      <td>-0.828548</td>\n",
       "      <td>-0.379002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267670</td>\n",
       "      <td>0.528057</td>\n",
       "      <td>-0.839760</td>\n",
       "      <td>0.145757</td>\n",
       "      <td>-0.588370</td>\n",
       "      <td>-0.210472</td>\n",
       "      <td>-0.893009</td>\n",
       "      <td>0.681746</td>\n",
       "      <td>-0.583680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1_35_0.mat</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.227591</td>\n",
       "      <td>0.153947</td>\n",
       "      <td>0.084576</td>\n",
       "      <td>-0.012415</td>\n",
       "      <td>-0.793800</td>\n",
       "      <td>-0.535874</td>\n",
       "      <td>-0.922401</td>\n",
       "      <td>-0.461207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.778602</td>\n",
       "      <td>0.371935</td>\n",
       "      <td>-0.856794</td>\n",
       "      <td>-0.612509</td>\n",
       "      <td>-0.842576</td>\n",
       "      <td>-0.210533</td>\n",
       "      <td>-0.963752</td>\n",
       "      <td>-0.334457</td>\n",
       "      <td>-1.048721</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1_36_0.mat</td>\n",
       "      <td>0.014622</td>\n",
       "      <td>0.267002</td>\n",
       "      <td>0.193806</td>\n",
       "      <td>0.084313</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>-0.860927</td>\n",
       "      <td>-0.568061</td>\n",
       "      <td>-1.032923</td>\n",
       "      <td>-0.425938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.664348</td>\n",
       "      <td>0.554392</td>\n",
       "      <td>-0.852656</td>\n",
       "      <td>-0.583196</td>\n",
       "      <td>-0.809238</td>\n",
       "      <td>-0.210530</td>\n",
       "      <td>-0.959351</td>\n",
       "      <td>-0.345511</td>\n",
       "      <td>-0.956040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>3_1531t_1.mat</td>\n",
       "      <td>0.011671</td>\n",
       "      <td>0.431802</td>\n",
       "      <td>0.048987</td>\n",
       "      <td>-0.096647</td>\n",
       "      <td>-0.165005</td>\n",
       "      <td>0.212099</td>\n",
       "      <td>-0.354532</td>\n",
       "      <td>-0.320580</td>\n",
       "      <td>-0.688015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194402</td>\n",
       "      <td>1.166286</td>\n",
       "      <td>-0.808954</td>\n",
       "      <td>-0.810540</td>\n",
       "      <td>-0.811545</td>\n",
       "      <td>-0.209129</td>\n",
       "      <td>0.189778</td>\n",
       "      <td>-0.692674</td>\n",
       "      <td>-0.355774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>3_1579t_1.mat</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.489856</td>\n",
       "      <td>0.050515</td>\n",
       "      <td>-0.001204</td>\n",
       "      <td>-0.270419</td>\n",
       "      <td>-0.033033</td>\n",
       "      <td>-0.430369</td>\n",
       "      <td>-0.257696</td>\n",
       "      <td>-0.513899</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.630805</td>\n",
       "      <td>-0.918680</td>\n",
       "      <td>-0.814398</td>\n",
       "      <td>-1.006325</td>\n",
       "      <td>-0.953082</td>\n",
       "      <td>-0.209563</td>\n",
       "      <td>-0.242075</td>\n",
       "      <td>-0.957460</td>\n",
       "      <td>-0.851200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4944</th>\n",
       "      <td>3_1614t_1.mat</td>\n",
       "      <td>0.014309</td>\n",
       "      <td>0.444097</td>\n",
       "      <td>0.023592</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>-0.251250</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>-0.386786</td>\n",
       "      <td>-0.241410</td>\n",
       "      <td>-0.486995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.795393</td>\n",
       "      <td>-0.782308</td>\n",
       "      <td>-0.821218</td>\n",
       "      <td>-1.041337</td>\n",
       "      <td>-0.964052</td>\n",
       "      <td>-0.209722</td>\n",
       "      <td>-0.418226</td>\n",
       "      <td>-0.992876</td>\n",
       "      <td>-0.923502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4945</th>\n",
       "      <td>3_1646t_1.mat</td>\n",
       "      <td>0.016058</td>\n",
       "      <td>0.450273</td>\n",
       "      <td>0.025744</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>-0.270630</td>\n",
       "      <td>-0.059092</td>\n",
       "      <td>-0.446341</td>\n",
       "      <td>-0.238723</td>\n",
       "      <td>-0.454307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.811289</td>\n",
       "      <td>-1.056228</td>\n",
       "      <td>-0.788994</td>\n",
       "      <td>-1.049046</td>\n",
       "      <td>-0.920037</td>\n",
       "      <td>-0.209707</td>\n",
       "      <td>-0.381957</td>\n",
       "      <td>-0.998785</td>\n",
       "      <td>-0.907336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>3_1713t_1.mat</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>0.435727</td>\n",
       "      <td>0.012452</td>\n",
       "      <td>0.023397</td>\n",
       "      <td>-0.295186</td>\n",
       "      <td>-0.083803</td>\n",
       "      <td>-0.452932</td>\n",
       "      <td>-0.259843</td>\n",
       "      <td>-0.420214</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789041</td>\n",
       "      <td>-0.612966</td>\n",
       "      <td>-0.714294</td>\n",
       "      <td>-0.943865</td>\n",
       "      <td>-0.873585</td>\n",
       "      <td>-0.209777</td>\n",
       "      <td>-0.404932</td>\n",
       "      <td>-0.944017</td>\n",
       "      <td>-0.848147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>3_1729t_1.mat</td>\n",
       "      <td>0.015138</td>\n",
       "      <td>0.447328</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>-0.267454</td>\n",
       "      <td>-0.002327</td>\n",
       "      <td>-0.416544</td>\n",
       "      <td>-0.268329</td>\n",
       "      <td>-0.497933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.515733</td>\n",
       "      <td>-0.413069</td>\n",
       "      <td>-0.690847</td>\n",
       "      <td>-0.952519</td>\n",
       "      <td>-0.887105</td>\n",
       "      <td>-0.209572</td>\n",
       "      <td>-0.273248</td>\n",
       "      <td>-0.920882</td>\n",
       "      <td>-0.785399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>3_1776t_1.mat</td>\n",
       "      <td>0.643552</td>\n",
       "      <td>0.495565</td>\n",
       "      <td>-1.793152</td>\n",
       "      <td>0.745008</td>\n",
       "      <td>0.057963</td>\n",
       "      <td>-1.262764</td>\n",
       "      <td>-1.087351</td>\n",
       "      <td>-0.124073</td>\n",
       "      <td>-0.189222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227270</td>\n",
       "      <td>0.660917</td>\n",
       "      <td>-0.774484</td>\n",
       "      <td>-0.933231</td>\n",
       "      <td>-0.935741</td>\n",
       "      <td>-0.209385</td>\n",
       "      <td>0.021624</td>\n",
       "      <td>-0.842907</td>\n",
       "      <td>-0.562513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>3_1781t_1.mat</td>\n",
       "      <td>0.016320</td>\n",
       "      <td>0.532404</td>\n",
       "      <td>0.066785</td>\n",
       "      <td>-0.069383</td>\n",
       "      <td>-0.265954</td>\n",
       "      <td>-0.002557</td>\n",
       "      <td>-0.446422</td>\n",
       "      <td>-0.306697</td>\n",
       "      <td>-0.531142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189740</td>\n",
       "      <td>-0.636116</td>\n",
       "      <td>-0.608003</td>\n",
       "      <td>-0.882881</td>\n",
       "      <td>-0.822440</td>\n",
       "      <td>-0.209390</td>\n",
       "      <td>-0.111983</td>\n",
       "      <td>-0.853420</td>\n",
       "      <td>-0.678285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>3_1850t_1.mat</td>\n",
       "      <td>0.015695</td>\n",
       "      <td>0.475538</td>\n",
       "      <td>0.035680</td>\n",
       "      <td>-0.097338</td>\n",
       "      <td>-0.199991</td>\n",
       "      <td>0.114026</td>\n",
       "      <td>-0.393705</td>\n",
       "      <td>-0.341252</td>\n",
       "      <td>-0.725810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270293</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>-0.863917</td>\n",
       "      <td>-0.875584</td>\n",
       "      <td>-0.816148</td>\n",
       "      <td>-0.209302</td>\n",
       "      <td>0.051460</td>\n",
       "      <td>-0.808274</td>\n",
       "      <td>-0.503396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>3_1857t_1.mat</td>\n",
       "      <td>0.012743</td>\n",
       "      <td>0.546728</td>\n",
       "      <td>0.096901</td>\n",
       "      <td>-0.142587</td>\n",
       "      <td>-0.136821</td>\n",
       "      <td>0.235905</td>\n",
       "      <td>-0.375035</td>\n",
       "      <td>-0.331744</td>\n",
       "      <td>-0.780682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670492</td>\n",
       "      <td>0.583496</td>\n",
       "      <td>-0.750113</td>\n",
       "      <td>-0.737314</td>\n",
       "      <td>-0.773476</td>\n",
       "      <td>-0.209057</td>\n",
       "      <td>0.246440</td>\n",
       "      <td>-0.591979</td>\n",
       "      <td>-0.246566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>3_1863t_1.mat</td>\n",
       "      <td>0.016033</td>\n",
       "      <td>0.447227</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.039160</td>\n",
       "      <td>-0.291083</td>\n",
       "      <td>-0.094729</td>\n",
       "      <td>-0.448662</td>\n",
       "      <td>-0.244666</td>\n",
       "      <td>-0.436166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.839362</td>\n",
       "      <td>-0.770636</td>\n",
       "      <td>-0.735681</td>\n",
       "      <td>-0.977206</td>\n",
       "      <td>-0.883380</td>\n",
       "      <td>-0.209754</td>\n",
       "      <td>-0.370487</td>\n",
       "      <td>-0.945831</td>\n",
       "      <td>-0.867040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>3_1882t_1.mat</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.480397</td>\n",
       "      <td>0.061476</td>\n",
       "      <td>-0.064149</td>\n",
       "      <td>-0.201111</td>\n",
       "      <td>0.137455</td>\n",
       "      <td>-0.379013</td>\n",
       "      <td>-0.290168</td>\n",
       "      <td>-0.592276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201281</td>\n",
       "      <td>-0.061374</td>\n",
       "      <td>-0.772091</td>\n",
       "      <td>-0.969079</td>\n",
       "      <td>-0.924870</td>\n",
       "      <td>-0.209438</td>\n",
       "      <td>-0.100029</td>\n",
       "      <td>-0.876935</td>\n",
       "      <td>-0.685091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>3_1899t_1.mat</td>\n",
       "      <td>0.013151</td>\n",
       "      <td>0.481235</td>\n",
       "      <td>0.069691</td>\n",
       "      <td>-0.110289</td>\n",
       "      <td>-0.212035</td>\n",
       "      <td>0.091211</td>\n",
       "      <td>-0.444568</td>\n",
       "      <td>-0.333659</td>\n",
       "      <td>-0.719282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370350</td>\n",
       "      <td>1.832759</td>\n",
       "      <td>-0.685092</td>\n",
       "      <td>-0.753567</td>\n",
       "      <td>-0.683110</td>\n",
       "      <td>-0.209152</td>\n",
       "      <td>0.133105</td>\n",
       "      <td>-0.679459</td>\n",
       "      <td>-0.281799</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>3_1907t_1.mat</td>\n",
       "      <td>0.017863</td>\n",
       "      <td>0.515541</td>\n",
       "      <td>0.119867</td>\n",
       "      <td>-0.126110</td>\n",
       "      <td>-0.258159</td>\n",
       "      <td>0.049728</td>\n",
       "      <td>-0.432268</td>\n",
       "      <td>-0.324656</td>\n",
       "      <td>-0.735036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573843</td>\n",
       "      <td>1.675153</td>\n",
       "      <td>-0.703747</td>\n",
       "      <td>-0.777109</td>\n",
       "      <td>-0.721313</td>\n",
       "      <td>-0.209182</td>\n",
       "      <td>0.135499</td>\n",
       "      <td>-0.719360</td>\n",
       "      <td>-0.318956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>3_1918t_1.mat</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>0.656232</td>\n",
       "      <td>0.116451</td>\n",
       "      <td>-0.167300</td>\n",
       "      <td>-0.281685</td>\n",
       "      <td>0.117662</td>\n",
       "      <td>-0.377144</td>\n",
       "      <td>-0.343485</td>\n",
       "      <td>-0.794801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426449</td>\n",
       "      <td>0.283581</td>\n",
       "      <td>-0.713527</td>\n",
       "      <td>-0.883912</td>\n",
       "      <td>-0.899807</td>\n",
       "      <td>-0.209144</td>\n",
       "      <td>0.143465</td>\n",
       "      <td>-0.780149</td>\n",
       "      <td>-0.484427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>3_1931t_1.mat</td>\n",
       "      <td>0.016811</td>\n",
       "      <td>0.459419</td>\n",
       "      <td>0.016106</td>\n",
       "      <td>0.015265</td>\n",
       "      <td>-0.299317</td>\n",
       "      <td>-0.083370</td>\n",
       "      <td>-0.460064</td>\n",
       "      <td>-0.252774</td>\n",
       "      <td>-0.429165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.791082</td>\n",
       "      <td>-0.574483</td>\n",
       "      <td>-0.717227</td>\n",
       "      <td>-0.975385</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.209682</td>\n",
       "      <td>-0.293107</td>\n",
       "      <td>-0.925209</td>\n",
       "      <td>-0.828183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>3_1936t_1.mat</td>\n",
       "      <td>0.018283</td>\n",
       "      <td>0.457715</td>\n",
       "      <td>0.083240</td>\n",
       "      <td>-0.202662</td>\n",
       "      <td>-0.154257</td>\n",
       "      <td>0.192998</td>\n",
       "      <td>-0.419392</td>\n",
       "      <td>-0.290094</td>\n",
       "      <td>-0.751772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984236</td>\n",
       "      <td>1.955730</td>\n",
       "      <td>-0.741369</td>\n",
       "      <td>-0.780670</td>\n",
       "      <td>-0.746113</td>\n",
       "      <td>-0.209067</td>\n",
       "      <td>0.238766</td>\n",
       "      <td>-0.641554</td>\n",
       "      <td>-0.159084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>3_1947t_1.mat</td>\n",
       "      <td>0.016708</td>\n",
       "      <td>0.426772</td>\n",
       "      <td>0.031634</td>\n",
       "      <td>0.014075</td>\n",
       "      <td>-0.277770</td>\n",
       "      <td>-0.055299</td>\n",
       "      <td>-0.443167</td>\n",
       "      <td>-0.273499</td>\n",
       "      <td>-0.411498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.831998</td>\n",
       "      <td>-0.732016</td>\n",
       "      <td>-0.766402</td>\n",
       "      <td>-0.992613</td>\n",
       "      <td>-0.922825</td>\n",
       "      <td>-0.209784</td>\n",
       "      <td>-0.408101</td>\n",
       "      <td>-0.988862</td>\n",
       "      <td>-0.919626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>3_1972t_1.mat</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>0.501073</td>\n",
       "      <td>0.079459</td>\n",
       "      <td>-0.152270</td>\n",
       "      <td>-0.168046</td>\n",
       "      <td>0.272297</td>\n",
       "      <td>-0.321061</td>\n",
       "      <td>-0.291851</td>\n",
       "      <td>-0.700298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731757</td>\n",
       "      <td>2.849343</td>\n",
       "      <td>-0.603011</td>\n",
       "      <td>-0.593296</td>\n",
       "      <td>-0.714402</td>\n",
       "      <td>-0.208745</td>\n",
       "      <td>0.662993</td>\n",
       "      <td>-0.351991</td>\n",
       "      <td>0.260769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>3_1981t_1.mat</td>\n",
       "      <td>0.016796</td>\n",
       "      <td>0.472685</td>\n",
       "      <td>0.035628</td>\n",
       "      <td>-0.120229</td>\n",
       "      <td>-0.183355</td>\n",
       "      <td>0.122884</td>\n",
       "      <td>-0.380279</td>\n",
       "      <td>-0.332561</td>\n",
       "      <td>-0.690761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124896</td>\n",
       "      <td>0.743791</td>\n",
       "      <td>-0.794068</td>\n",
       "      <td>-0.817920</td>\n",
       "      <td>-0.851305</td>\n",
       "      <td>-0.209104</td>\n",
       "      <td>0.155972</td>\n",
       "      <td>-0.746610</td>\n",
       "      <td>-0.378295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>3_2039t_1.mat</td>\n",
       "      <td>0.017184</td>\n",
       "      <td>0.441355</td>\n",
       "      <td>0.016851</td>\n",
       "      <td>0.025769</td>\n",
       "      <td>-0.282046</td>\n",
       "      <td>-0.054232</td>\n",
       "      <td>-0.424167</td>\n",
       "      <td>-0.264242</td>\n",
       "      <td>-0.452738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765964</td>\n",
       "      <td>-0.857126</td>\n",
       "      <td>-0.822470</td>\n",
       "      <td>-1.049027</td>\n",
       "      <td>-0.966929</td>\n",
       "      <td>-0.209690</td>\n",
       "      <td>-0.386843</td>\n",
       "      <td>-1.037594</td>\n",
       "      <td>-0.903400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>3_2040t_1.mat</td>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.509096</td>\n",
       "      <td>0.045553</td>\n",
       "      <td>-0.133913</td>\n",
       "      <td>-0.209675</td>\n",
       "      <td>0.143867</td>\n",
       "      <td>-0.387051</td>\n",
       "      <td>-0.342570</td>\n",
       "      <td>-0.754238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762692</td>\n",
       "      <td>1.053558</td>\n",
       "      <td>-0.843541</td>\n",
       "      <td>-0.869379</td>\n",
       "      <td>-0.828940</td>\n",
       "      <td>-0.209166</td>\n",
       "      <td>0.053814</td>\n",
       "      <td>-0.737172</td>\n",
       "      <td>-0.320709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4964</th>\n",
       "      <td>3_2054t_1.mat</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.489954</td>\n",
       "      <td>0.060490</td>\n",
       "      <td>-0.103770</td>\n",
       "      <td>-0.187770</td>\n",
       "      <td>0.216617</td>\n",
       "      <td>-0.377419</td>\n",
       "      <td>-0.359567</td>\n",
       "      <td>-0.761025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355912</td>\n",
       "      <td>1.171572</td>\n",
       "      <td>-0.834582</td>\n",
       "      <td>-0.879232</td>\n",
       "      <td>-0.866144</td>\n",
       "      <td>-0.209220</td>\n",
       "      <td>0.068528</td>\n",
       "      <td>-0.814244</td>\n",
       "      <td>-0.468465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965</th>\n",
       "      <td>3_2106t_1.mat</td>\n",
       "      <td>0.016254</td>\n",
       "      <td>0.515319</td>\n",
       "      <td>0.084085</td>\n",
       "      <td>-0.122419</td>\n",
       "      <td>-0.259017</td>\n",
       "      <td>0.052031</td>\n",
       "      <td>-0.398749</td>\n",
       "      <td>-0.319324</td>\n",
       "      <td>-0.682119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439744</td>\n",
       "      <td>0.639744</td>\n",
       "      <td>-0.808870</td>\n",
       "      <td>-0.849673</td>\n",
       "      <td>-0.766997</td>\n",
       "      <td>-0.209114</td>\n",
       "      <td>0.135686</td>\n",
       "      <td>-0.778515</td>\n",
       "      <td>-0.384613</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966</th>\n",
       "      <td>3_2108t_1.mat</td>\n",
       "      <td>0.016030</td>\n",
       "      <td>0.464873</td>\n",
       "      <td>0.042905</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>-0.277038</td>\n",
       "      <td>-0.044454</td>\n",
       "      <td>-0.442038</td>\n",
       "      <td>-0.269737</td>\n",
       "      <td>-0.483820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.791518</td>\n",
       "      <td>-0.846147</td>\n",
       "      <td>-0.777906</td>\n",
       "      <td>-1.036307</td>\n",
       "      <td>-0.951840</td>\n",
       "      <td>-0.209646</td>\n",
       "      <td>-0.360966</td>\n",
       "      <td>-1.004660</td>\n",
       "      <td>-0.942438</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>3_2127t_1.mat</td>\n",
       "      <td>0.015376</td>\n",
       "      <td>0.418669</td>\n",
       "      <td>0.053826</td>\n",
       "      <td>-0.066764</td>\n",
       "      <td>-0.256558</td>\n",
       "      <td>0.019290</td>\n",
       "      <td>-0.418290</td>\n",
       "      <td>-0.324326</td>\n",
       "      <td>-0.667564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200479</td>\n",
       "      <td>0.788592</td>\n",
       "      <td>-0.785744</td>\n",
       "      <td>-0.867218</td>\n",
       "      <td>-0.866541</td>\n",
       "      <td>-0.209375</td>\n",
       "      <td>-0.010732</td>\n",
       "      <td>-0.807341</td>\n",
       "      <td>-0.531878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>3_2148t_1.mat</td>\n",
       "      <td>0.016389</td>\n",
       "      <td>0.660114</td>\n",
       "      <td>0.113011</td>\n",
       "      <td>-0.166070</td>\n",
       "      <td>-0.271742</td>\n",
       "      <td>0.126749</td>\n",
       "      <td>-0.386000</td>\n",
       "      <td>-0.364062</td>\n",
       "      <td>-0.744138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655880</td>\n",
       "      <td>0.367676</td>\n",
       "      <td>-0.702193</td>\n",
       "      <td>-0.871429</td>\n",
       "      <td>-0.886198</td>\n",
       "      <td>-0.209112</td>\n",
       "      <td>0.141473</td>\n",
       "      <td>-0.753213</td>\n",
       "      <td>-0.355606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>3_2157t_1.mat</td>\n",
       "      <td>0.015396</td>\n",
       "      <td>0.485148</td>\n",
       "      <td>0.033510</td>\n",
       "      <td>-0.057753</td>\n",
       "      <td>-0.274070</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>-0.403926</td>\n",
       "      <td>-0.289852</td>\n",
       "      <td>-0.599467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271178</td>\n",
       "      <td>0.430745</td>\n",
       "      <td>-0.576630</td>\n",
       "      <td>-0.924411</td>\n",
       "      <td>-0.876843</td>\n",
       "      <td>-0.209462</td>\n",
       "      <td>-0.149630</td>\n",
       "      <td>-0.875111</td>\n",
       "      <td>-0.749493</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>3_2169t_1.mat</td>\n",
       "      <td>0.016099</td>\n",
       "      <td>0.547981</td>\n",
       "      <td>0.083592</td>\n",
       "      <td>-0.060115</td>\n",
       "      <td>-0.272057</td>\n",
       "      <td>-0.009254</td>\n",
       "      <td>-0.426379</td>\n",
       "      <td>-0.326527</td>\n",
       "      <td>-0.591194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427867</td>\n",
       "      <td>0.293200</td>\n",
       "      <td>-0.663788</td>\n",
       "      <td>-0.691480</td>\n",
       "      <td>-0.854107</td>\n",
       "      <td>-0.208869</td>\n",
       "      <td>0.439069</td>\n",
       "      <td>-0.409616</td>\n",
       "      <td>0.215338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>3_2245t_1.mat</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>0.496801</td>\n",
       "      <td>0.103567</td>\n",
       "      <td>-0.143702</td>\n",
       "      <td>-0.300849</td>\n",
       "      <td>0.045782</td>\n",
       "      <td>-0.442188</td>\n",
       "      <td>-0.284534</td>\n",
       "      <td>-0.653355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600225</td>\n",
       "      <td>0.765314</td>\n",
       "      <td>-0.724579</td>\n",
       "      <td>-0.738261</td>\n",
       "      <td>-0.692938</td>\n",
       "      <td>-0.209034</td>\n",
       "      <td>0.227076</td>\n",
       "      <td>-0.636488</td>\n",
       "      <td>-0.175344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4972 rows × 850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               File  SigMean_C1  SigMean_C2  SigMean_C3  SigMean_C4  \\\n",
       "0         1_7_0.mat   -0.288530    0.324564   -0.002718    0.050880   \n",
       "1         1_8_0.mat   -0.048981   -0.093086   -0.123211    0.576776   \n",
       "2         1_9_0.mat   -0.168991    0.137907    0.106642   -0.239069   \n",
       "3        1_10_0.mat   -0.199351   -0.573721   -0.527654    0.060454   \n",
       "4        1_11_0.mat    0.017845    0.010380   -0.022627    0.094499   \n",
       "5        1_12_0.mat    0.014489    0.130260    0.082293    0.071109   \n",
       "6        1_13_0.mat   -0.655157    0.457431    0.140928    0.072871   \n",
       "7        1_14_0.mat    0.576938   -0.021378    1.319599   -0.323885   \n",
       "8        1_15_0.mat    0.508465   -0.365782   -0.653625   -0.290081   \n",
       "9        1_16_0.mat   -0.017030    0.316921    0.026681    0.078928   \n",
       "10       1_17_0.mat   -0.107355   -0.162953   -0.996098    2.808183   \n",
       "11       1_18_0.mat    2.468389    0.736495   -0.318129    2.140369   \n",
       "12       1_19_0.mat    0.016263    0.226342    0.117856    0.140571   \n",
       "13       1_20_0.mat    0.001564    0.240421    0.070596    0.148317   \n",
       "14       1_21_0.mat    0.086422    0.327529   -0.013377   -0.019369   \n",
       "15       1_22_0.mat    0.014130    0.231487    0.116156    0.041383   \n",
       "16       1_23_0.mat    0.016582    0.251332    0.109356    0.120858   \n",
       "17       1_24_0.mat   -5.874305    7.124888   -4.263323   -0.992420   \n",
       "18       1_25_0.mat    2.389926   -3.164038    0.647671   -3.416691   \n",
       "19       1_26_0.mat    0.122764    0.057910    0.075340    0.180366   \n",
       "20       1_27_0.mat   -0.049447    0.354053    0.457940    0.352845   \n",
       "21       1_28_0.mat   -0.978077   -4.088753   -1.917240   -5.834190   \n",
       "22       1_29_0.mat    0.807601   -1.820035    0.401598    0.706551   \n",
       "23       1_30_0.mat    2.150922   -1.918457    0.436620    1.746149   \n",
       "24       1_31_0.mat   -0.059716   -0.413812    0.005706    0.167308   \n",
       "25       1_32_0.mat   -0.238290    0.199344    0.008342    0.479131   \n",
       "26       1_33_0.mat    0.017021    0.248512    0.180592    0.094865   \n",
       "27       1_34_0.mat    0.010845    0.257040    0.077783    0.115970   \n",
       "28       1_35_0.mat    0.016704    0.227591    0.153947    0.084576   \n",
       "29       1_36_0.mat    0.014622    0.267002    0.193806    0.084313   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "4942  3_1531t_1.mat    0.011671    0.431802    0.048987   -0.096647   \n",
       "4943  3_1579t_1.mat    0.017151    0.489856    0.050515   -0.001204   \n",
       "4944  3_1614t_1.mat    0.014309    0.444097    0.023592    0.008614   \n",
       "4945  3_1646t_1.mat    0.016058    0.450273    0.025744    0.026875   \n",
       "4946  3_1713t_1.mat    0.013331    0.435727    0.012452    0.023397   \n",
       "4947  3_1729t_1.mat    0.015138    0.447328    0.016105    0.000464   \n",
       "4948  3_1776t_1.mat    0.643552    0.495565   -1.793152    0.745008   \n",
       "4949  3_1781t_1.mat    0.016320    0.532404    0.066785   -0.069383   \n",
       "4950  3_1850t_1.mat    0.015695    0.475538    0.035680   -0.097338   \n",
       "4951  3_1857t_1.mat    0.012743    0.546728    0.096901   -0.142587   \n",
       "4952  3_1863t_1.mat    0.016033    0.447227    0.015151    0.039160   \n",
       "4953  3_1882t_1.mat    0.012284    0.480397    0.061476   -0.064149   \n",
       "4954  3_1899t_1.mat    0.013151    0.481235    0.069691   -0.110289   \n",
       "4955  3_1907t_1.mat    0.017863    0.515541    0.119867   -0.126110   \n",
       "4956  3_1918t_1.mat    0.015333    0.656232    0.116451   -0.167300   \n",
       "4957  3_1931t_1.mat    0.016811    0.459419    0.016106    0.015265   \n",
       "4958  3_1936t_1.mat    0.018283    0.457715    0.083240   -0.202662   \n",
       "4959  3_1947t_1.mat    0.016708    0.426772    0.031634    0.014075   \n",
       "4960  3_1972t_1.mat    0.013271    0.501073    0.079459   -0.152270   \n",
       "4961  3_1981t_1.mat    0.016796    0.472685    0.035628   -0.120229   \n",
       "4962  3_2039t_1.mat    0.017184    0.441355    0.016851    0.025769   \n",
       "4963  3_2040t_1.mat    0.014346    0.509096    0.045553   -0.133913   \n",
       "4964  3_2054t_1.mat    0.012925    0.489954    0.060490   -0.103770   \n",
       "4965  3_2106t_1.mat    0.016254    0.515319    0.084085   -0.122419   \n",
       "4966  3_2108t_1.mat    0.016030    0.464873    0.042905    0.016192   \n",
       "4967  3_2127t_1.mat    0.015376    0.418669    0.053826   -0.066764   \n",
       "4968  3_2148t_1.mat    0.016389    0.660114    0.113011   -0.166070   \n",
       "4969  3_2157t_1.mat    0.015396    0.485148    0.033510   -0.057753   \n",
       "4970  3_2169t_1.mat    0.016099    0.547981    0.083592   -0.060115   \n",
       "4971  3_2245t_1.mat    0.015581    0.496801    0.103567   -0.143702   \n",
       "\n",
       "      SigMean_C5  SigMean_C6  SigMean_C7  SigMean_C8  SigMean_C9  ...    \\\n",
       "0       0.146694   -0.736724   -0.265787   -0.800872   -0.454978  ...     \n",
       "1      -0.059393   -0.626494   -0.288666   -1.123369   -1.058181  ...     \n",
       "2      -0.695586   -1.335152   -0.421489   -1.152721   -0.869702  ...     \n",
       "3      -0.342314   -1.070089   -0.899636   -1.362659   -1.266849  ...     \n",
       "4      -0.046953   -0.936635   -0.546479   -0.991634   -0.663843  ...     \n",
       "5      -0.102388   -1.003618   -0.422200   -1.093781   -0.668133  ...     \n",
       "6      -0.366381   -1.075830   -0.403581   -1.694735   -0.391248  ...     \n",
       "7      -1.541729   -0.644021   -0.693023   -3.264855    1.226745  ...     \n",
       "8       0.490278    0.147102   -0.857894   -0.593983   -0.182086  ...     \n",
       "9      -0.083321   -0.622562   -0.509687   -0.653605   -0.235385  ...     \n",
       "10     -0.174293   -1.528328   -2.178322   -0.614406    0.757732  ...     \n",
       "11     -1.028426   -1.562106   -1.514582   -1.777203   -0.097207  ...     \n",
       "12     -0.037668   -0.728453   -0.520898   -0.852833   -0.396602  ...     \n",
       "13     -0.090425   -0.735969   -0.485500   -0.761253   -0.357012  ...     \n",
       "14     -0.114642   -0.624669   -0.486112   -0.737054   -0.382490  ...     \n",
       "15     -0.093172   -0.889774   -0.514569   -1.004461   -0.467471  ...     \n",
       "16     -0.052773   -0.742272   -0.494545   -0.866886   -0.374134  ...     \n",
       "17     -2.913278    1.678971    4.676792   -5.520978   -3.742057  ...     \n",
       "18     -1.308395   -1.356191   -3.886885   -2.056498    3.684275  ...     \n",
       "19     -0.290268   -1.185950   -0.712845   -1.475375   -0.622130  ...     \n",
       "20     -0.045770   -1.091065   -0.909598   -0.971723   -0.615520  ...     \n",
       "21      0.823131   -3.208397    2.904948   -2.675808   -2.962324  ...     \n",
       "22     -0.017233   -0.653581   -0.482750   -1.127154   -0.552759  ...     \n",
       "23     -1.079286   -1.321718   -0.367232   -2.488519   -0.347563  ...     \n",
       "24     -0.307473   -1.267701   -0.479681   -1.133853   -0.908891  ...     \n",
       "25     -1.533048   -1.264341    0.169659   -1.351542   -0.741736  ...     \n",
       "26     -0.019913   -0.762517   -0.533509   -0.889742   -0.404175  ...     \n",
       "27     -0.022243   -0.745458   -0.510756   -0.828548   -0.379002  ...     \n",
       "28     -0.012415   -0.793800   -0.535874   -0.922401   -0.461207  ...     \n",
       "29      0.001312   -0.860927   -0.568061   -1.032923   -0.425938  ...     \n",
       "...          ...         ...         ...         ...         ...  ...     \n",
       "4942   -0.165005    0.212099   -0.354532   -0.320580   -0.688015  ...     \n",
       "4943   -0.270419   -0.033033   -0.430369   -0.257696   -0.513899  ...     \n",
       "4944   -0.251250   -0.000822   -0.386786   -0.241410   -0.486995  ...     \n",
       "4945   -0.270630   -0.059092   -0.446341   -0.238723   -0.454307  ...     \n",
       "4946   -0.295186   -0.083803   -0.452932   -0.259843   -0.420214  ...     \n",
       "4947   -0.267454   -0.002327   -0.416544   -0.268329   -0.497933  ...     \n",
       "4948    0.057963   -1.262764   -1.087351   -0.124073   -0.189222  ...     \n",
       "4949   -0.265954   -0.002557   -0.446422   -0.306697   -0.531142  ...     \n",
       "4950   -0.199991    0.114026   -0.393705   -0.341252   -0.725810  ...     \n",
       "4951   -0.136821    0.235905   -0.375035   -0.331744   -0.780682  ...     \n",
       "4952   -0.291083   -0.094729   -0.448662   -0.244666   -0.436166  ...     \n",
       "4953   -0.201111    0.137455   -0.379013   -0.290168   -0.592276  ...     \n",
       "4954   -0.212035    0.091211   -0.444568   -0.333659   -0.719282  ...     \n",
       "4955   -0.258159    0.049728   -0.432268   -0.324656   -0.735036  ...     \n",
       "4956   -0.281685    0.117662   -0.377144   -0.343485   -0.794801  ...     \n",
       "4957   -0.299317   -0.083370   -0.460064   -0.252774   -0.429165  ...     \n",
       "4958   -0.154257    0.192998   -0.419392   -0.290094   -0.751772  ...     \n",
       "4959   -0.277770   -0.055299   -0.443167   -0.273499   -0.411498  ...     \n",
       "4960   -0.168046    0.272297   -0.321061   -0.291851   -0.700298  ...     \n",
       "4961   -0.183355    0.122884   -0.380279   -0.332561   -0.690761  ...     \n",
       "4962   -0.282046   -0.054232   -0.424167   -0.264242   -0.452738  ...     \n",
       "4963   -0.209675    0.143867   -0.387051   -0.342570   -0.754238  ...     \n",
       "4964   -0.187770    0.216617   -0.377419   -0.359567   -0.761025  ...     \n",
       "4965   -0.259017    0.052031   -0.398749   -0.319324   -0.682119  ...     \n",
       "4966   -0.277038   -0.044454   -0.442038   -0.269737   -0.483820  ...     \n",
       "4967   -0.256558    0.019290   -0.418290   -0.324326   -0.667564  ...     \n",
       "4968   -0.271742    0.126749   -0.386000   -0.364062   -0.744138  ...     \n",
       "4969   -0.274070    0.004565   -0.403926   -0.289852   -0.599467  ...     \n",
       "4970   -0.272057   -0.009254   -0.426379   -0.326527   -0.591194  ...     \n",
       "4971   -0.300849    0.045782   -0.442188   -0.284534   -0.653355  ...     \n",
       "\n",
       "      WaveletR8_C8  WaveletR8_C9  WaveletR8_C10  WaveletR8_C11  WaveletR8_C12  \\\n",
       "0         0.394613      1.075273      -0.866328      -0.078471      -0.640318   \n",
       "1         3.960875      3.582588      -0.785167       1.371965      -0.095059   \n",
       "2         4.068216      3.496801      -0.770978       1.984995       0.122791   \n",
       "3         3.214353      3.345645      -0.776387       1.197794      -0.173821   \n",
       "4         4.601992      3.607907      -0.779511       1.731818      -0.051429   \n",
       "5         5.240694      4.073852      -0.767021       1.792692      -0.015426   \n",
       "6         5.449587      2.865873      -0.748247       1.190386      -0.160518   \n",
       "7         5.731851      4.206076      -0.651534       2.545810       0.387698   \n",
       "8        -0.408528      0.264590      -0.894032      -0.706963      -0.830039   \n",
       "9        -1.645358     -0.752254      -0.931867      -1.095465      -0.994557   \n",
       "10       -0.248843     -0.743139      -0.915971      -0.976023      -0.937788   \n",
       "11       -1.216453     -0.389211      -0.854222      -0.603501      -0.828734   \n",
       "12        0.339248      0.314408      -0.836682      -0.219681      -0.651738   \n",
       "13       -0.248185      0.373849      -0.856493      -0.219434      -0.694672   \n",
       "14       -0.176888      0.011213      -0.836551      -0.381501      -0.700757   \n",
       "15        0.682073      1.264365      -0.824499       0.301450      -0.554697   \n",
       "16       -0.293572      0.761325      -0.845750       0.022369      -0.624567   \n",
       "17       -0.389553      0.127452      -0.892874      -0.529728      -0.675885   \n",
       "18        5.688695      1.067532      -0.638198       2.786467       0.128534   \n",
       "19        5.094197      3.242722      -0.554116       3.723095       0.684206   \n",
       "20        3.860770      2.812556      -0.704434       2.430364       0.364384   \n",
       "21        4.308685      2.604854      -0.683105       2.233156       0.305950   \n",
       "22        0.555284      1.417922      -0.811425      -0.516092      -0.663596   \n",
       "23        0.578934      0.796345      -0.819136       0.081394      -0.585357   \n",
       "24        1.819994      2.680907      -0.730192       1.843782      -0.099635   \n",
       "25        0.564346      0.866977      -0.803806       0.315601      -0.495856   \n",
       "26       -1.038268      0.297366      -0.877518      -0.724054      -0.852111   \n",
       "27       -0.267670      0.528057      -0.839760       0.145757      -0.588370   \n",
       "28       -0.778602      0.371935      -0.856794      -0.612509      -0.842576   \n",
       "29       -0.664348      0.554392      -0.852656      -0.583196      -0.809238   \n",
       "...            ...           ...            ...            ...            ...   \n",
       "4942      0.194402      1.166286      -0.808954      -0.810540      -0.811545   \n",
       "4943     -0.630805     -0.918680      -0.814398      -1.006325      -0.953082   \n",
       "4944     -0.795393     -0.782308      -0.821218      -1.041337      -0.964052   \n",
       "4945     -0.811289     -1.056228      -0.788994      -1.049046      -0.920037   \n",
       "4946     -0.789041     -0.612966      -0.714294      -0.943865      -0.873585   \n",
       "4947     -0.515733     -0.413069      -0.690847      -0.952519      -0.887105   \n",
       "4948      0.227270      0.660917      -0.774484      -0.933231      -0.935741   \n",
       "4949     -0.189740     -0.636116      -0.608003      -0.882881      -0.822440   \n",
       "4950      0.270293      0.995310      -0.863917      -0.875584      -0.816148   \n",
       "4951      0.670492      0.583496      -0.750113      -0.737314      -0.773476   \n",
       "4952     -0.839362     -0.770636      -0.735681      -0.977206      -0.883380   \n",
       "4953     -0.201281     -0.061374      -0.772091      -0.969079      -0.924870   \n",
       "4954      0.370350      1.832759      -0.685092      -0.753567      -0.683110   \n",
       "4955      0.573843      1.675153      -0.703747      -0.777109      -0.721313   \n",
       "4956      0.426449      0.283581      -0.713527      -0.883912      -0.899807   \n",
       "4957     -0.791082     -0.574483      -0.717227      -0.975385      -0.888064   \n",
       "4958      0.984236      1.955730      -0.741369      -0.780670      -0.746113   \n",
       "4959     -0.831998     -0.732016      -0.766402      -0.992613      -0.922825   \n",
       "4960      0.731757      2.849343      -0.603011      -0.593296      -0.714402   \n",
       "4961      0.124896      0.743791      -0.794068      -0.817920      -0.851305   \n",
       "4962     -0.765964     -0.857126      -0.822470      -1.049027      -0.966929   \n",
       "4963      0.762692      1.053558      -0.843541      -0.869379      -0.828940   \n",
       "4964      0.355912      1.171572      -0.834582      -0.879232      -0.866144   \n",
       "4965      0.439744      0.639744      -0.808870      -0.849673      -0.766997   \n",
       "4966     -0.791518     -0.846147      -0.777906      -1.036307      -0.951840   \n",
       "4967      0.200479      0.788592      -0.785744      -0.867218      -0.866541   \n",
       "4968      0.655880      0.367676      -0.702193      -0.871429      -0.886198   \n",
       "4969     -0.271178      0.430745      -0.576630      -0.924411      -0.876843   \n",
       "4970      0.427867      0.293200      -0.663788      -0.691480      -0.854107   \n",
       "4971      0.600225      0.765314      -0.724579      -0.738261      -0.692938   \n",
       "\n",
       "      WaveletR8_C13  WaveletR8_C14  WaveletR8_C15  WaveletR8_C16  Class  \n",
       "0         -0.210430      -0.824698       0.523578      -0.091089      0  \n",
       "1         -0.210288      -0.517351       3.076091       1.487048      0  \n",
       "2         -0.209740       0.028066       3.771308       2.205124      0  \n",
       "3         -0.210032      -0.429824       2.981368       1.241364      0  \n",
       "4         -0.210167      -0.369933       4.052383       2.052193      0  \n",
       "5         -0.210365      -0.517251       3.994455       2.011985      0  \n",
       "6         -0.209625       0.293477       2.675517       2.167857      0  \n",
       "7         -0.209287       0.332244       4.419004       2.935790      0  \n",
       "8         -0.210508      -0.961791      -0.352436      -0.911083      0  \n",
       "9         -0.210540      -1.031693      -1.033472      -1.365559      0  \n",
       "10        -0.210552      -1.006010      -0.932202      -1.320866      0  \n",
       "11        -0.210379      -0.799011      -0.341136      -0.910694      0  \n",
       "12        -0.210252      -0.649826       0.389490      -0.092823      0  \n",
       "13        -0.210431      -0.812970       0.357022      -0.515895      0  \n",
       "14        -0.210453      -0.860743       0.273623      -0.483924      0  \n",
       "15        -0.210486      -0.858859       1.086474      -0.419907      0  \n",
       "16        -0.210493      -0.892961       0.592680      -0.672328      0  \n",
       "17        -0.209785       0.197999       0.054220      -0.017420      0  \n",
       "18        -0.209757       0.127087       4.108325       2.613091      0  \n",
       "19        -0.210126      -0.361573       5.107177       2.191459      0  \n",
       "20        -0.209966      -0.272638       4.633073       2.003624      0  \n",
       "21        -0.209304      -0.140386       4.158798       2.974046      0  \n",
       "22        -0.210252      -0.745527       0.149602      -0.260921      0  \n",
       "23        -0.210318      -0.654840       0.823207      -0.201391      0  \n",
       "24        -0.210265      -0.586212       3.195324       0.761748      0  \n",
       "25        -0.210234      -0.672642       1.242084      -0.114290      0  \n",
       "26        -0.210530      -0.969573      -0.630685      -1.106383      0  \n",
       "27        -0.210472      -0.893009       0.681746      -0.583680      0  \n",
       "28        -0.210533      -0.963752      -0.334457      -1.048721      0  \n",
       "29        -0.210530      -0.959351      -0.345511      -0.956040      0  \n",
       "...             ...            ...            ...            ...    ...  \n",
       "4942      -0.209129       0.189778      -0.692674      -0.355774      1  \n",
       "4943      -0.209563      -0.242075      -0.957460      -0.851200      1  \n",
       "4944      -0.209722      -0.418226      -0.992876      -0.923502      1  \n",
       "4945      -0.209707      -0.381957      -0.998785      -0.907336      1  \n",
       "4946      -0.209777      -0.404932      -0.944017      -0.848147      1  \n",
       "4947      -0.209572      -0.273248      -0.920882      -0.785399      1  \n",
       "4948      -0.209385       0.021624      -0.842907      -0.562513      1  \n",
       "4949      -0.209390      -0.111983      -0.853420      -0.678285      1  \n",
       "4950      -0.209302       0.051460      -0.808274      -0.503396      1  \n",
       "4951      -0.209057       0.246440      -0.591979      -0.246566      1  \n",
       "4952      -0.209754      -0.370487      -0.945831      -0.867040      1  \n",
       "4953      -0.209438      -0.100029      -0.876935      -0.685091      1  \n",
       "4954      -0.209152       0.133105      -0.679459      -0.281799      1  \n",
       "4955      -0.209182       0.135499      -0.719360      -0.318956      1  \n",
       "4956      -0.209144       0.143465      -0.780149      -0.484427      1  \n",
       "4957      -0.209682      -0.293107      -0.925209      -0.828183      1  \n",
       "4958      -0.209067       0.238766      -0.641554      -0.159084      1  \n",
       "4959      -0.209784      -0.408101      -0.988862      -0.919626      1  \n",
       "4960      -0.208745       0.662993      -0.351991       0.260769      1  \n",
       "4961      -0.209104       0.155972      -0.746610      -0.378295      1  \n",
       "4962      -0.209690      -0.386843      -1.037594      -0.903400      1  \n",
       "4963      -0.209166       0.053814      -0.737172      -0.320709      1  \n",
       "4964      -0.209220       0.068528      -0.814244      -0.468465      1  \n",
       "4965      -0.209114       0.135686      -0.778515      -0.384613      1  \n",
       "4966      -0.209646      -0.360966      -1.004660      -0.942438      1  \n",
       "4967      -0.209375      -0.010732      -0.807341      -0.531878      1  \n",
       "4968      -0.209112       0.141473      -0.753213      -0.355606      1  \n",
       "4969      -0.209462      -0.149630      -0.875111      -0.749493      1  \n",
       "4970      -0.208869       0.439069      -0.409616       0.215338      1  \n",
       "4971      -0.209034       0.227076      -0.636488      -0.175344      1  \n",
       "\n",
       "[4972 rows x 850 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "groups = pd.read_csv('../input/group.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7_0.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_8_0.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_9_0.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_10_0.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_11_0.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1_12_0.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1_13_0.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1_14_0.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1_15_0.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1_16_0.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1_17_0.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1_18_0.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1_19_0.mat</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1_20_0.mat</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1_21_0.mat</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1_22_0.mat</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1_23_0.mat</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1_24_0.mat</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1_25_0.mat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1_26_0.mat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1_27_0.mat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1_28_0.mat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1_29_0.mat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1_30_0.mat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1_31_0.mat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1_32_0.mat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1_33_0.mat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1_34_0.mat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1_35_0.mat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1_36_0.mat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>3_1531t_1.mat</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>3_1579t_1.mat</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4944</th>\n",
       "      <td>3_1614t_1.mat</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4945</th>\n",
       "      <td>3_1646t_1.mat</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>3_1713t_1.mat</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>3_1729t_1.mat</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>3_1776t_1.mat</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>3_1781t_1.mat</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>3_1850t_1.mat</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>3_1857t_1.mat</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>3_1863t_1.mat</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>3_1882t_1.mat</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>3_1899t_1.mat</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>3_1907t_1.mat</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>3_1918t_1.mat</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>3_1931t_1.mat</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>3_1936t_1.mat</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>3_1947t_1.mat</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>3_1972t_1.mat</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>3_1981t_1.mat</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>3_2039t_1.mat</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>3_2040t_1.mat</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4964</th>\n",
       "      <td>3_2054t_1.mat</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965</th>\n",
       "      <td>3_2106t_1.mat</td>\n",
       "      <td>2046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966</th>\n",
       "      <td>3_2108t_1.mat</td>\n",
       "      <td>2046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>3_2127t_1.mat</td>\n",
       "      <td>2046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>3_2148t_1.mat</td>\n",
       "      <td>2046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>3_2157t_1.mat</td>\n",
       "      <td>2046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>3_2169t_1.mat</td>\n",
       "      <td>2046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>3_2245t_1.mat</td>\n",
       "      <td>2047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               File  Group\n",
       "0         1_7_0.mat      1\n",
       "1         1_8_0.mat      1\n",
       "2         1_9_0.mat      1\n",
       "3        1_10_0.mat      1\n",
       "4        1_11_0.mat      1\n",
       "5        1_12_0.mat      1\n",
       "6        1_13_0.mat      2\n",
       "7        1_14_0.mat      2\n",
       "8        1_15_0.mat      2\n",
       "9        1_16_0.mat      2\n",
       "10       1_17_0.mat      2\n",
       "11       1_18_0.mat      2\n",
       "12       1_19_0.mat      3\n",
       "13       1_20_0.mat      3\n",
       "14       1_21_0.mat      3\n",
       "15       1_22_0.mat      3\n",
       "16       1_23_0.mat      3\n",
       "17       1_24_0.mat      3\n",
       "18       1_25_0.mat      4\n",
       "19       1_26_0.mat      4\n",
       "20       1_27_0.mat      4\n",
       "21       1_28_0.mat      4\n",
       "22       1_29_0.mat      4\n",
       "23       1_30_0.mat      4\n",
       "24       1_31_0.mat      5\n",
       "25       1_32_0.mat      5\n",
       "26       1_33_0.mat      5\n",
       "27       1_34_0.mat      5\n",
       "28       1_35_0.mat      5\n",
       "29       1_36_0.mat      5\n",
       "...             ...    ...\n",
       "4942  3_1531t_1.mat   2042\n",
       "4943  3_1579t_1.mat   2042\n",
       "4944  3_1614t_1.mat   2042\n",
       "4945  3_1646t_1.mat   2042\n",
       "4946  3_1713t_1.mat   2042\n",
       "4947  3_1729t_1.mat   2043\n",
       "4948  3_1776t_1.mat   2043\n",
       "4949  3_1781t_1.mat   2043\n",
       "4950  3_1850t_1.mat   2043\n",
       "4951  3_1857t_1.mat   2043\n",
       "4952  3_1863t_1.mat   2043\n",
       "4953  3_1882t_1.mat   2044\n",
       "4954  3_1899t_1.mat   2044\n",
       "4955  3_1907t_1.mat   2044\n",
       "4956  3_1918t_1.mat   2044\n",
       "4957  3_1931t_1.mat   2044\n",
       "4958  3_1936t_1.mat   2044\n",
       "4959  3_1947t_1.mat   2045\n",
       "4960  3_1972t_1.mat   2045\n",
       "4961  3_1981t_1.mat   2045\n",
       "4962  3_2039t_1.mat   2045\n",
       "4963  3_2040t_1.mat   2045\n",
       "4964  3_2054t_1.mat   2045\n",
       "4965  3_2106t_1.mat   2046\n",
       "4966  3_2108t_1.mat   2046\n",
       "4967  3_2127t_1.mat   2046\n",
       "4968  3_2148t_1.mat   2046\n",
       "4969  3_2157t_1.mat   2046\n",
       "4970  3_2169t_1.mat   2046\n",
       "4971  3_2245t_1.mat   2047\n",
       "\n",
       "[4972 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python -W ignore::DeprecationWarning\n",
    "from sklearn.model_selection import GroupShuffleSplit, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, mdl_type, param):\n",
    "        \"\"\" Initialize classififer based on classifier type and related parameters\n",
    "\n",
    "        Arguments:\n",
    "            mdl_type (string): a string to tell which classifier to use\n",
    "            param: a dictionary to store related parameters\n",
    "            \n",
    "        \"\"\"\n",
    "        self.param = param\n",
    "        self.mdl_type = mdl_type\n",
    "        if (mdl_type == 'xgb'):\n",
    "            self.clf = xgb.XGBClassifier(**param)\n",
    "        elif (mdl_type == 'lr'):\n",
    "            self.clf = LogisticRegression(**param)\n",
    "        elif (mdl_type == 'extra'):\n",
    "            self.clf = ExtraTreesClassifier(**param)\n",
    "        elif (mdl_type == 'rf'):\n",
    "            self.clf = RandomForestClassifier(**param)\n",
    "\n",
    "    # training the model based on input data\n",
    "    def train(self, data_in, data_out):\n",
    "        \"\"\" Train classifier based on input data and corresponding labels\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "        \"\"\"\n",
    "        if(self.mdl_type=='xgb'):\n",
    "            self.clf.fit(data_in, data_out, eval_metric='auc')\n",
    "        else:\n",
    "            self.clf.fit(data_in, data_out)\n",
    "\n",
    "    # split into k fold and find auc for each fold (for testing purpose)\n",
    "    def score_kfold(self, data_in, data_out, ns):\n",
    "        \"\"\" Get the score of k-fold cross validation\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "            num_fold: number of folds for cross validation \n",
    "        \n",
    "        Returns:\n",
    "            val_mean: mean of k-fold cross validation score\n",
    "            val_std: standard deviation of k-fold cross validation score\n",
    "        \"\"\"\n",
    "        # cut into k-folds\n",
    "        kf = KFold(n_splits=ns, shuffle=True)\n",
    "        self.score = []\n",
    "        for train_idx, test_idx in kf.split(data_in, y=data_out):\n",
    "            # divide into trainig and test set\n",
    "            train_in = data_in[train_idx, :]\n",
    "            train_out = data_out[train_idx]\n",
    "            test_in = data_in[test_idx, :]\n",
    "            test_out = data_out[test_idx]\n",
    "\n",
    "            # fit model\n",
    "            self.train(train_in, train_out)\n",
    "            test_pred = self.predict(test_in)\n",
    "\n",
    "            # get score\n",
    "            self.score.append(self.get_score(test_out, test_pred))\n",
    "\n",
    "        # return score mean and std\n",
    "        print(self.score)\n",
    "        return np.mean(self.score), np.std(self.score)\n",
    "\n",
    "    def score_group_kfold(self, data_in, data_out, groups, ns):\n",
    "        \"\"\" Get the score of group cross validation\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "            groups (ndarray): group information of input data\n",
    "            ns: number of splits for cross validation\n",
    "        \n",
    "        Returns:\n",
    "            val_mean: mean of k-fold cross validation score\n",
    "            val_std: standard deviation of k-fold cross validation score\n",
    "        \"\"\"\n",
    "        gkf = GroupShuffleSplit(n_splits=ns, random_state=0)\n",
    "        self.score = []\n",
    "        for train_idx, test_idx in gkf.split(data_in, y=data_out, groups=groups):\n",
    "            # divide into trainig and test set\n",
    "            train_in = data_in[train_idx, :]\n",
    "            train_out = data_out[train_idx]\n",
    "            test_in = data_in[test_idx, :]\n",
    "            test_out = data_out[test_idx]\n",
    "\n",
    "            # fit model\n",
    "            self.train(train_in, train_out)\n",
    "            test_pred = self.predict(test_in)\n",
    "\n",
    "            # get score\n",
    "            self.score.append(self.get_score(test_out, test_pred))\n",
    "\n",
    "        # return score mean and std\n",
    "        return np.mean(self.score), np.std(self.score)\n",
    "\n",
    "    # prediction with probability outcome\n",
    "    def predict(self, data_in):\n",
    "        data_out = self.clf.predict_proba(data_in)[:, 1]\n",
    "        return data_out\n",
    "\n",
    "    # get the ROC curve\n",
    "    def get_score(self, test_out, test_pred):\n",
    "        fpr, tpr, _ = roc_curve(test_out, test_pred)\n",
    "        return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.790837948072 0.0463663931922\n"
     ]
    }
   ],
   "source": [
    "# use group to improve cross validation\n",
    "MyData = Datawarehouse()\n",
    "MyData.read_data()\n",
    "\n",
    "# setup model parameters\n",
    "param = {'C':1,'penalty':'l1','max_iter':400,'n_jobs':8}\n",
    "MyModelLR = Model('lr', param)\n",
    "\n",
    "# perform cross validation\n",
    "val_mean, val_std = MyModelLR.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "print(val_mean, val_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823487085659 0.0345062174434\n"
     ]
    }
   ],
   "source": [
    "# test random forest\n",
    "param = {'n_estimators': 800,'criterion':'entropy','max_features':'sqrt','max_depth':10, \\\n",
    "        'min_samples_split':8, 'random_state':4242, 'n_jobs':8}\n",
    "MyModelRF = Model('rf', param)\n",
    "\n",
    "val_mean, val_std = MyModelRF.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "print(val_mean, val_std)\n",
    "\n",
    "MyModelRF.train(MyData.train_in, MyData.train_out)\n",
    "ypred = MyModelRF.predict(MyData.test_in)\n",
    "MyData.gen_submission(ypred, 'submission_rf_all_features.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use l1-regularization to select features\n",
    "MyModelLR.train(MyData.train_in, MyData.train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE6ZJREFUeJzt3Xu0pXV93/H3x+EqsARkykWQIYlllVIDdoJarVrECMRb\nbF3F1ltrM7WrWQltWgLSWm3jWqaxrrYrTVKMRpcX1HjDIi6BxjZLEzEzZKDDTbmZ4TqjgiKayOXb\nP57nwOZwZs6Zs585+zm/836ttdfsvZ9nP7/P2TPzOb/ze55zTqoKSVI7njLrAJKkYVnsktQYi12S\nGmOxS1JjLHZJaozFLkmNsdjVvCS/l+TfzzqHtFLidezalSS3A0cCj0w8/der6q4pjvkS4KNVdex0\n6VanJB8C7qiqfzfrLGqXM3Yt5pVVdfDEbdmlPoQk+8xy/GkkWTfrDFobLHYtS5LnJfmTJPcnuaaf\nic9t+ydJbkjyQJJbk/zz/vmDgC8BxyT5YX87JsmHkvzGxOtfkuSOice3J/n1JNcCDybZp3/dZ5Ls\nTHJbkl/ZTdbHjj937CTnJdmR5O4kr0lydpJvJvlekrdPvPadST6d5JP9x3N1kp+d2P43kvyf/n24\nLsmr5o37u0kuS/Ig8FbgHwPn9R/7/+r3Oz/JLf3xr0/yixPHeEuSryZ5b5L7+o/1rInthyf5gyR3\n9ds/P7HtFUm29tn+JMmzJ7b9epI7+zFvSvLSJfy1a7WoKm/eFrwBtwNnLPD8M4DvAmfTTQ5e1j9e\n32//BeCngQAvBn4EPKff9hK6pYjJ430I+I2Jx0/Yp8+xFTgOOLAfcwvwDmA/4KeAW4GX7+LjeOz4\n/bEf7l+7L/BLwE7g48AhwN8Efgyc0O//TuAh4B/0+/8b4Lb+/r7AzcDb+xynAw8AJ06M+33gBX3m\nA+Z/rP1+rwOO6ff5h8CDwNH9trf04/8SsA74F8BdPL6M+kXgk8BhfZ4X98+fCuwAntu/7s39+7g/\ncCKwHTim33cD8NOz/vfmbbibM3Yt5vP9jO/+idngG4DLquqyqnq0qq4ANtMVPVX1xaq6pTr/F7gc\n+LtT5vjvVbW9qn4M/BzdJ5H/WFU/qapbgfcD5yzxWA8B766qh4BPAEcA/62qHqiq64DrgZ+d2H9L\nVX263/99dAX9vP52MPCePscfAZcCr5947SVV9bX+ffrLhcJU1R9W1V39Pp8EvgWcNrHLt6vq/VX1\nCPBh4GjgyCRHA2cBb6uq+6rqof79BtgE/M+quqqqHqmqDwN/1Wd+hK7gT0qyb1XdXlW3LPG90ypg\nsWsxr6mqQ/vba/rnjgdeN1H49wMvpCsckpyV5Ov9ssb9dIV/xJQ5tk/cP55uOWdy/LfTnehdiu/2\nJQnd7Bzg3ontP6Yr7CeNXVWPAnfQzbCPAbb3z835Nt1XNAvlXlCSN00smdwPnMwT3697Jsb/UX/3\nYLqvYL5XVfctcNjjgV+b9x4dRzdLvxk4l+6rkR1JPpHkmMVyavWw2LUc24GPTBT+oVV1UFW9J8n+\nwGeA9wJHVtWhwGV0yzIAC12G9SDw1InHRy2wz+TrtgO3zRv/kKo6e+qPbGHHzd1J8hTgWLrlkLuA\n4/rn5jwTuHMXuZ/0OMnxdF9t/DLw9P792sbj79fubAcOT3LoLra9e9579NSquhigqj5eVS+k+wRQ\nwG8uYTytEha7luOjwCuTvDzJuiQH9Cclj6Vba96fbt364f5E389PvPZe4OlJnjbx3Fbg7P5E4FF0\ns8nd+QbwQH8C8MA+w8lJfm6wj/CJ/naS16a7IudcuiWNrwNX0Z0/OC/Jvv0J5FfSLe/syr105wTm\nHERXrDuhO/FMN2NfVFXdTXcy+neSHNZneFG/+f3A25I8N52DkvxCkkOSnJjk9P6T8F/SfYXy6C6G\n0SpksWuPVdV24NV0yx876WaH/xZ4SlU9APwK8CngPuAfAV+YeO2NwMXArf0SwTHAR4Br6E7uXU53\nMnB34z8CvAI4he5E5neA3weetrvXTeESupOa9wFvBF7br2f/hK7Iz+oz/A7wpv5j3JUP0K1t35/k\n81V1PfBfgD+lK/2/BXxtD7K9ke6cwY10J0vPBaiqzXQnXH+7z30z3YlY6D7xvqfPfA/w14AL9mBM\njZzfoCTtRpJ3Aj9TVW+YdRZpqZyxS1JjLHZJaoxLMZLUGGfsktSYmfxApSOOOKI2bNgwi6EladXa\nsmXLd6pq/WL7zaTYN2zYwObNm2cxtCStWkm+vZT9XIqRpMZY7JLUGItdkhpjsUtSYyx2SWqMxS5J\njbHYJakxFrskNcZil6TGWOyStBpkKb8tsWOxS1JjLHZJaozFLkmNsdglqTFTF3uSA5J8I8k1Sa5L\n8q4hgkmSlmeIn8f+V8DpVfXDJPsCX03ypar6+gDHliTtoamLvbpfmvrD/uG+/c1fpCpJMzLIGnuS\ndUm2AjuAK6rqqgX22ZRkc5LNO3fuHGJYSdICBin2qnqkqk4BjgVOS3LyAvtcVFUbq2rj+vWL/so+\nSdIyDXpVTFXdD3wFOHPI40qSlm6Iq2LWJzm0v38g8DLgxmmPK0nq7cGPE4Bhroo5GvhwknV0nyg+\nVVWXDnBcSdIyDHFVzLXAqQNkkSQNwO88laTGWOyS1BiLXZIaY7FLUmMsdklqjMUuSY2x2CWpMRa7\nJDXGYpekxljsktQYi12SGmOxS1JjLHZJaozFLkmNsdglacz28JdsgMUuSc2x2CWpMRa7JDXGYpek\nxljsktSYqYs9yXFJvpLk+iTXJfnVIYJJkpZnnwGO8TDwa1V1dZJDgC1Jrqiq6wc4tiRpD009Y6+q\nu6vq6v7+A8ANwDOmPa4kaXkGXWNPsgE4FbhqgW2bkmxOsnnnzp1DDitJmjBYsSc5GPgMcG5V/WD+\n9qq6qKo2VtXG9evXDzWsJGmeQYo9yb50pf6xqvrsEMeUJC3PEFfFBPgAcENVvW/6SJIkYFk/JwaG\nmbG/AHgjcHqSrf3t7AGOK0lahqkvd6yqrwLL+7QiSVrYMmfr4HeeSlJzLHZJGpspZutgsUtScyx2\nSWqMxS5JjbHYJakxFrskNcZil6TGWOyS1BiLXZIaY7FLUmMsdklqjMUuSY2x2CWpMRa7JI3JlD8A\nDCx2SRqPAUodLHZJao7FLkljMNBsHSx2SWqOxS5JszTgTH2OxS5JjRmk2JN8MMmOJNuGOJ4kafmG\nmrF/CDhzoGNJUrvmll72whLMnEGKvar+GPjeEMeSJE1nxdbYk2xKsjnJ5p07d67UsJI0Hntxlj5p\nxYq9qi6qqo1VtXH9+vUrNawkjcMKlTp4VYwk7X0rWOoA+6zoaJK0VqxwmU8a6nLHi4E/BU5MckeS\ntw5xXElalWZY6jDQjL2qXj/EcSRJ03ONXZKGMuOZ+hyLXZKmNZJCn2OxS1JjLHZJWq6RzdTnWOyS\ntBwjLXXwOnZJ2jMjLvQ5ztglaalWQamDxS5JS7NKSh1cipGk3VtFhT7HYpekhazCQp/jUowkTVrF\nhT7HYpekxrgUI2ltm5yhV80ux4AsdklrTwPLLbtjsUtaGxov80musUtq3xoqdbDYJbVsjRX6HItd\nUnvWaKHPcY1d0urX4JUt07DYJa1ea3xmviuDLMUkOTPJTUluTnL+EMeUpCdInnzTgqYu9iTrgP8B\nnAWcBLw+yUnTHlfSGjdX3Bb4Hhtixn4acHNV3VpVPwE+Abx6gONKWkucjQ9miDX2ZwDbJx7fATx3\n/k5JNgGbAJ7ZPdGd5Jj8cynmv2aa1+6t8Zab0ZM+Wst29e9/7vk9+f8x/zXTvHZvjbec1yyxt1bs\ncsequqiqNlbVxvUrNehqUPX4TZIGMMSM/U7guInHx/bPaXcsckl7yRAz9j8DnpXkhCT7AecAXxjg\nuO1xdi5pBUw9Y6+qh5P8MvBlYB3wwaq6bupkrVjOep0kTWGQb1CqqsuAy4Y41qpnkUuaMb/zdCgW\nuaSRsNiXyyKXNFL+dMc94TKLpFXAYl8Ki1zSKuJSzEIsckmrmMU+yUKX1ACLHSx0SU1Z28VuoUtq\nkCdPJakxa7fYna1LatTaK3YLXVLj1sYau2UuaQ1pu9gtdElrUHvFbplLWuPaKXYLXZKAtXjyVJIa\n10axO1uXpMe0UeySpMes7mJ3pi5JT7K6i12S9CRTFXuS1yW5LsmjSTYOFUqStHzTzti3Aa8F/niA\nLEvnEowk7dJU17FX1Q0ASYZJI0mammvsktSYRWfsSa4Ejlpg04VVdclSB0qyCdgE8Mwlx5Mk7alF\ni72qzhhioKq6CLgIYGOyvEVy19YlaVEuxUhSY6a93PEXk9wBPB/4YpIvDxNLkrRc014V8zngcwNl\nWWywFRlGkla71bEUY6lL0pKNv9gtdUnaI+MvdknSHhnvb1Bypi5Jy+KMXZIaM74ZuzN1SZrKuGbs\nlrokTW1cxS5Jmtp4it3ZuiQNYjzFLkkahMUuSY2x2CWpMRa7JDXGYpekxljsktQYi12SGmOxS1Jj\nLHZJaozFLkmNsdglqTEWuyQ1ZqpiT/JbSW5Mcm2SzyU5dKhgkqTlmXbGfgVwclU9G/gmcMH0kSRJ\n05iq2Kvq8qp6uH/4deDY6SNJkqYx5Br7PwW+tKuNSTYl2Zxk887JDf4cdkka1KK/8zTJlcBRC2y6\nsKou6fe5EHgY+NiujlNVFwEXAWxMbHNJ2ksWLfaqOmN325O8BXgF8NIqp9+SNGuLFvvuJDkTOA94\ncVX9aJhIkqRpTLvG/tvAIcAVSbYm+b0BMkmSpjDVjL2qfmaoIJKkYfidp5LUGItdkhpjsUtSYyx2\nSWqMxS5JjbHYJakxFrskNcZil6TGWOyS1BiLXZIaY7FLUmMsdklqjMUuSY2x2CWpMRa7JDVmdsXu\nb9GTpL3CGbskNcZil6TGWOyS1BiLXZIaM1WxJ/lPSa5NsjXJ5UmOGSqYJGl5pp2x/1ZVPbuqTgEu\nBd4xQCZJ0hSmKvaq+sHEw4MAr2GUpBnbZ9oDJHk38Cbg+8Df281+m4BNAM+cdlBJ0i6lFvlGoSRX\nAkctsOnCqrpkYr8LgAOq6j8sNujGpDb7DUqStEeSbKmqjYvtt+iMvarOWOKYHwMuAxYtdknS3jPt\nVTHPmnj4auDG6eJIkqY17Rr7e5KcCDwKfBt42/SRJEnTmKrYq+rvDxVEkjQMv/NUkhpjsUtSYyx2\nSWqMxS5JjbHYJakxFrskNcZil6TGWOyS1BiLXZIaY7FLUmMsdklqjMUuSY2x2CWpMRa7JDXGYpek\nxljsktQYi12SGpOqWvlBkweAm1Z84KU7AvjOrEPswpizwbjzjTkbjDuf2ZZvyHzHV9X6xXaa9nee\nLtdNVbVxRmMvKsnmseYbczYYd74xZ4Nx5zPb8s0in0sxktQYi12SGjOrYr9oRuMu1ZjzjTkbjDvf\nmLPBuPOZbflWPN9MTp5KkvYel2IkqTEWuyQ1ZsWLPcmZSW5KcnOS82cw/geT7EiybeK5w5NckeRb\n/Z+HTWy7oM96U5KX7+VsxyX5SpLrk1yX5FdHlu+AJN9Ick2f711jytePty7Jnye5dITZbk/y/5Js\nTbJ5TPmSHJrk00luTHJDkuePKNuJ/Xs2d/tBknNHlO9f9f8ftiW5uP9/MttsVbViN2AdcAvwU8B+\nwDXASSuc4UXAc4BtE8/9Z+D8/v75wG/290/qM+4PnNBnX7cXsx0NPKe/fwjwzT7DWPIFOLi/vy9w\nFfC8seTrx/zXwMeBS8f0d9uPeTtwxLznRpEP+DDwz/r7+wGHjiXbvJzrgHuA48eQD3gGcBtwYP/4\nU8BbZp1tr/9FzHsTng98eeLxBcAFK5mhH3cDTyz2m4Cj+/tH030D1ZPyAV8Gnr+COS8BXjbGfMBT\ngauB544lH3As8L+B03m82EeRrR/jdp5c7DPPBzytL6eMLdsCWX8e+NpY8tEV+3bgcLpv+Ly0zzjT\nbCu9FDP3Jsy5o39u1o6sqrv7+/cAR/b3Z5Y3yQbgVLpZ8Wjy9UsdW4EdwBVVNaZ8/xU4D3h04rmx\nZAMo4MokW5JsGlG+E4CdwB/0y1i/n+SgkWSb7xzg4v7+zPNV1Z3Ae4G/AO4Gvl9Vl886mydP56nu\n0+hMrwFNcjDwGeDcqvrB5LZZ56uqR6rqFLrZ8WlJTp63fSb5krwC2FFVW3a1z6zfO+CF/Xt3FvAv\nk7xocuMM8+1Dtzz5u1V1KvAg3fLBGLI9Jsl+wKuAP5y/bYb/7g4DXk33yfEY4KAkb5h1tpUu9juB\n4yYeH9s/N2v3JjkaoP9zR//8iudNsi9dqX+sqj47tnxzqup+4CvAmSPJ9wLgVUluBz4BnJ7koyPJ\nBjw2u6OqdgCfA04bSb47gDv6r74APk1X9GPINuks4Oqqurd/PIZ8ZwC3VdXOqnoI+Czwd2adbaWL\n/c+AZyU5of/sew7whRXOsJAvAG/u77+Zbm177vlzkuyf5ATgWcA39laIJAE+ANxQVe8bYb71SQ7t\n7x9It/5/4xjyVdUFVXVsVW2g+3f1R1X1hjFkA0hyUJJD5u7TrcNuG0O+qroH2J7kxP6plwLXjyHb\nPK/n8WWYuRyzzvcXwPOSPLX///tS4IaZZ1uJEx7zTjacTXe1xy3AhTMY/2K6tbCH6GYqbwWeTnfS\n7VvAlcDhE/tf2Ge9CThrL2d7Id2XbNcCW/vb2SPK92zgz/t824B39M+PIt/EmC/h8ZOno8hGdyXY\nNf3turl/+yPKdwqwuf+7/Txw2Fiy9eMdBHwXeNrEc6PIB7yLboKzDfgI3RUvM83mjxSQpMZ48lSS\nGmOxS1JjLHZJaozFLkmNsdglqTEWuyQ1xmKXpMb8f7lw6h+m9YmlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c56bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot the feature importance\n",
    "importances = MyModelLR.clf.coef_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(MyData.train_in.shape[1]), importances[0,indices[0]], color=\"r\",  align=\"center\")\n",
    "plt.xlim([-1, MyData.train_in.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.76305591e+00,  -3.61695525e+00,  -3.24342717e+00,\n",
       "        -2.70861281e+00,  -2.22561631e+00,  -2.21808631e+00,\n",
       "        -2.13047449e+00,  -2.02752110e+00,  -1.99775898e+00,\n",
       "        -1.85427720e+00,  -1.65121206e+00,  -1.61221594e+00,\n",
       "        -1.56681804e+00,  -1.44923689e+00,  -1.42343606e+00,\n",
       "        -1.41720932e+00,  -1.40187012e+00,  -1.39896926e+00,\n",
       "        -1.35978388e+00,  -1.35455225e+00,  -1.34988138e+00,\n",
       "        -1.34230062e+00,  -1.33034461e+00,  -1.30337225e+00,\n",
       "        -1.30026131e+00,  -1.28888709e+00,  -1.28421327e+00,\n",
       "        -1.27626922e+00,  -1.20916990e+00,  -1.19312375e+00,\n",
       "        -1.18055645e+00,  -1.17455154e+00,  -1.16750410e+00,\n",
       "        -1.15281065e+00,  -1.11246233e+00,  -1.08561836e+00,\n",
       "        -1.04209685e+00,  -1.02095062e+00,  -9.90626683e-01,\n",
       "        -9.56921236e-01,  -9.34505500e-01,  -9.17005374e-01,\n",
       "        -8.98487952e-01,  -8.96282141e-01,  -8.68388104e-01,\n",
       "        -8.04282635e-01,  -8.02609929e-01,  -7.95469397e-01,\n",
       "        -7.93094178e-01,  -7.67279977e-01,  -7.42966054e-01,\n",
       "        -7.34104373e-01,  -7.33501116e-01,  -7.08028446e-01,\n",
       "        -6.79606938e-01,  -6.75432893e-01,  -6.74408996e-01,\n",
       "        -6.72476460e-01,  -6.64459034e-01,  -6.56613632e-01,\n",
       "        -6.54703201e-01,  -6.53738564e-01,  -6.42280126e-01,\n",
       "        -6.37374986e-01,  -6.33648799e-01,  -6.16924661e-01,\n",
       "        -6.08419883e-01,  -6.07598185e-01,  -6.06691797e-01,\n",
       "        -5.97674996e-01,  -5.90612085e-01,  -5.90088877e-01,\n",
       "        -5.86972590e-01,  -5.84450455e-01,  -5.76687310e-01,\n",
       "        -5.74323521e-01,  -5.64149360e-01,  -5.56794008e-01,\n",
       "        -5.49027922e-01,  -5.37065883e-01,  -5.36295309e-01,\n",
       "        -5.23512336e-01,  -5.14656232e-01,  -5.10843489e-01,\n",
       "        -5.03193957e-01,  -4.87990571e-01,  -4.54174183e-01,\n",
       "        -4.52344656e-01,  -4.51948388e-01,  -4.33721003e-01,\n",
       "        -4.12748890e-01,  -4.10878216e-01,  -4.02144589e-01,\n",
       "        -4.01086633e-01,  -3.90955836e-01,  -3.85864442e-01,\n",
       "        -3.80237908e-01,  -3.78069762e-01,  -3.77059344e-01,\n",
       "        -3.72781078e-01,  -3.72203817e-01,  -3.67346565e-01,\n",
       "        -3.52298403e-01,  -3.37827041e-01,  -3.35189150e-01,\n",
       "        -3.32285272e-01,  -3.21528258e-01,  -3.19801854e-01,\n",
       "        -3.17306766e-01,  -3.10643653e-01,  -2.96476709e-01,\n",
       "        -2.90179001e-01,  -2.87195808e-01,  -2.85493632e-01,\n",
       "        -2.84595775e-01,  -2.82834733e-01,  -2.78012560e-01,\n",
       "        -2.72023253e-01,  -2.69283724e-01,  -2.64092070e-01,\n",
       "        -2.61663277e-01,  -2.58941386e-01,  -2.56391278e-01,\n",
       "        -2.55282030e-01,  -2.53076067e-01,  -2.47674394e-01,\n",
       "        -2.42808142e-01,  -2.37256672e-01,  -2.26535568e-01,\n",
       "        -2.25658216e-01,  -2.22605218e-01,  -2.19689414e-01,\n",
       "        -2.19395854e-01,  -2.16736912e-01,  -2.07593300e-01,\n",
       "        -2.02332368e-01,  -1.93844448e-01,  -1.89018690e-01,\n",
       "        -1.87920834e-01,  -1.78015685e-01,  -1.77498859e-01,\n",
       "        -1.77050703e-01,  -1.65423947e-01,  -1.63679592e-01,\n",
       "        -1.54503905e-01,  -1.51568559e-01,  -1.46016227e-01,\n",
       "        -1.44137705e-01,  -1.39187263e-01,  -1.36618426e-01,\n",
       "        -1.35290361e-01,  -1.30218720e-01,  -1.29883209e-01,\n",
       "        -1.22443630e-01,  -1.21835762e-01,  -1.20972905e-01,\n",
       "        -1.19388114e-01,  -1.16761837e-01,  -1.12429337e-01,\n",
       "        -1.08401354e-01,  -1.04750429e-01,  -9.33982784e-02,\n",
       "        -9.10605330e-02,  -8.77511655e-02,  -8.10922847e-02,\n",
       "        -7.92268000e-02,  -6.40706336e-02,  -5.42922873e-02,\n",
       "        -4.99340130e-02,  -4.61420118e-02,  -4.38333795e-02,\n",
       "        -4.23305767e-02,  -3.26370201e-02,  -3.06407084e-02,\n",
       "        -2.94252530e-02,  -2.19777310e-02,  -1.38978282e-02,\n",
       "        -1.32872846e-02,  -1.24705349e-02,  -3.66865475e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.79027596e-03,\n",
       "         8.33889791e-03,   8.42376524e-03,   1.17483938e-02,\n",
       "         1.49346261e-02,   1.87463286e-02,   1.96216880e-02,\n",
       "         2.62474141e-02,   3.21480377e-02,   3.28487236e-02,\n",
       "         3.28869057e-02,   4.03716157e-02,   4.08674749e-02,\n",
       "         4.11485906e-02,   4.26028748e-02,   4.54000667e-02,\n",
       "         4.99325221e-02,   6.24546077e-02,   6.39623739e-02,\n",
       "         6.48323668e-02,   6.64301443e-02,   6.77492517e-02,\n",
       "         7.08263193e-02,   7.18094203e-02,   7.49741054e-02,\n",
       "         8.01831804e-02,   8.32179580e-02,   8.81824830e-02,\n",
       "         9.23097389e-02,   9.31796082e-02,   9.50198910e-02,\n",
       "         9.86959527e-02,   9.94431206e-02,   1.00283882e-01,\n",
       "         1.12900753e-01,   1.14056535e-01,   1.18548136e-01,\n",
       "         1.19708953e-01,   1.22751809e-01,   1.23222305e-01,\n",
       "         1.34988006e-01,   1.43114894e-01,   1.46654191e-01,\n",
       "         1.47307744e-01,   1.48332476e-01,   1.57673413e-01,\n",
       "         1.71962189e-01,   1.76236424e-01,   1.77867925e-01,\n",
       "         1.78882409e-01,   1.79419813e-01,   1.87645473e-01,\n",
       "         1.88075524e-01,   2.09210371e-01,   2.11375297e-01,\n",
       "         2.17411555e-01,   2.19138782e-01,   2.19768278e-01,\n",
       "         2.21053170e-01,   2.25736284e-01,   2.28474956e-01,\n",
       "         2.32378858e-01,   2.38916507e-01,   2.39620580e-01,\n",
       "         2.43619556e-01,   2.49903127e-01,   2.56867446e-01,\n",
       "         2.63384218e-01,   2.67711435e-01,   2.94119999e-01,\n",
       "         3.01610087e-01,   3.04392394e-01,   3.06563926e-01,\n",
       "         3.15662676e-01,   3.18772065e-01,   3.20131615e-01,\n",
       "         3.22029466e-01,   3.31513402e-01,   3.35343007e-01,\n",
       "         3.41162458e-01,   3.44247258e-01,   3.53916252e-01,\n",
       "         3.54354182e-01,   3.56128964e-01,   3.62880120e-01,\n",
       "         3.66029194e-01,   3.69457736e-01,   3.72858498e-01,\n",
       "         3.75563872e-01,   3.75804457e-01,   3.87462931e-01,\n",
       "         3.90781430e-01,   3.94111752e-01,   4.01919689e-01,\n",
       "         4.06616596e-01,   4.32850563e-01,   4.36977548e-01,\n",
       "         4.41653945e-01,   4.43152518e-01,   4.43568602e-01,\n",
       "         4.65454654e-01,   4.75258587e-01,   4.78231156e-01,\n",
       "         4.87324375e-01,   5.03563204e-01,   5.16420448e-01,\n",
       "         5.17465341e-01,   5.20686604e-01,   5.30901785e-01,\n",
       "         5.38393080e-01,   5.57159237e-01,   5.62911036e-01,\n",
       "         5.67108952e-01,   5.71770672e-01,   5.79171696e-01,\n",
       "         6.02067367e-01,   6.02832106e-01,   6.09305427e-01,\n",
       "         6.24626401e-01,   6.34827335e-01,   6.53046081e-01,\n",
       "         6.69318600e-01,   6.72360837e-01,   6.88275088e-01,\n",
       "         6.90826459e-01,   7.39112316e-01,   7.41178043e-01,\n",
       "         7.42369346e-01,   7.49475554e-01,   7.51274731e-01,\n",
       "         7.54956151e-01,   7.62921909e-01,   7.81823929e-01,\n",
       "         8.01241703e-01,   8.03953694e-01,   8.16375839e-01,\n",
       "         8.21188730e-01,   8.29561163e-01,   8.49216891e-01,\n",
       "         8.63584591e-01,   8.66298452e-01,   8.71730192e-01,\n",
       "         8.84151038e-01,   8.93284212e-01,   8.97438583e-01,\n",
       "         8.98351528e-01,   9.03475430e-01,   9.09539700e-01,\n",
       "         9.15018662e-01,   9.35447483e-01,   9.37580661e-01,\n",
       "         9.68057415e-01,   9.68383055e-01,   9.88239704e-01,\n",
       "         1.01783929e+00,   1.02300386e+00,   1.04651661e+00,\n",
       "         1.04879673e+00,   1.05499779e+00,   1.05991031e+00,\n",
       "         1.06249841e+00,   1.06684109e+00,   1.06763282e+00,\n",
       "         1.06784383e+00,   1.09088205e+00,   1.09799383e+00,\n",
       "         1.17599737e+00,   1.17754114e+00,   1.18430854e+00,\n",
       "         1.20871834e+00,   1.21982372e+00,   1.22088681e+00,\n",
       "         1.26198478e+00,   1.27080923e+00,   1.30910450e+00,\n",
       "         1.31317031e+00,   1.31862104e+00,   1.41187857e+00,\n",
       "         1.76200420e+00,   1.77372178e+00,   1.81037352e+00,\n",
       "         1.84231814e+00,   1.84767500e+00,   1.87957221e+00,\n",
       "         1.90934036e+00,   2.17199155e+00,   2.32254395e+00,\n",
       "         2.51684001e+00,   2.71323462e+00,   2.88881793e+00,\n",
       "         2.92937643e+00,   3.27820042e+00])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances[0][indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TimeCorr_C6_C3', 'FreqCorr_C15_C15', 'TimeCorr_C14_C10',\n",
       "       'DyadicL12_C9', 'WaveletR5_C10', 'SigVar_C1', 'FreqCorr_C12_C6',\n",
       "       'TimeCorr_C8_C5', 'DyadicL8_C4', 'WaveletR8_C7', 'DyadicL5_C16',\n",
       "       'WaveletR2_C1', 'WaveletR7_C5', 'DyadicL10_C3', 'WaveletR1_C14',\n",
       "       'TimeCorr_C12_C8', 'DyadicL7_C13', 'spectralEdgeFreq_C15',\n",
       "       'WaveletR3_C12', 'deltaPower_C5', 'FreqCorr_C10_C4', 'WaveletR5_C16',\n",
       "       'WaveletR7_C7', 'WaveletR2_C8', 'DyadicL10_C10', 'highGammaPower_C16',\n",
       "       'Hurst_C10', 'TimeCorr_C15_C7', 'DyadicL4_C8', 'DyadicL7_C11',\n",
       "       'DyadicL3_C16', 'WaveletR4_C16', 'FreqCorr_C10_C2', 'TimeCorr_C6_C4',\n",
       "       'TimeEig_Rank1', 'TimeCorr_C12_C2', 'DyadicL5_C2', 'FreqCorr_C14_C6',\n",
       "       'FreqCorr_C16_C8', 'TimeEig_Rank4', 'DyadicL2_C6', 'TimeCorr_C15_C13',\n",
       "       'DyadicL7_C1', 'DyadicL9_C4', 'DyadicL2_C2', 'DyadicL1_C12',\n",
       "       'DyadicL11_C13', 'spectralEntropy_C4', 'DyadicL1_C11',\n",
       "       'lowGammaPower_C4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyData.col_name[indices][0,-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Eliminate un-used features\n",
    "KeepVec = (importances!=0)\n",
    "MyData.select_features(KeepVec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4972, 372)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyData.train_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822743800815 0.0329929555428\n"
     ]
    }
   ],
   "source": [
    "# retrain random forest classifier with reduced features\n",
    "param = {'n_estimators': 800,'criterion':'entropy','max_features':'sqrt','max_depth':10,\\\n",
    "        'min_samples_split':8, 'random_state':4242, 'n_jobs':8}\n",
    "MyModelRF = Model('rf', param)\n",
    "val_mean, val_std = MyModelRF.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "print(val_mean, val_std)\n",
    "\n",
    "# Train classifier using all the data\n",
    "MyModelRF.train(MyData.train_in, MyData.train_out)\n",
    "ypred = MyModelRF.predict(MyData.test_in)\n",
    "MyData.gen_submission(ypred, 'submission_rf_selected_features.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Parameter tuning\n",
    "* Use logistic regression as example\n",
    "* Play with Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "MyData = Datawarehouse()\n",
    "MyData.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def score_lr(C, max_iter):\n",
    "    \"\"\" return score of an individual experiment\n",
    "    \"\"\"\n",
    "    param = {'penalty':'l1','n_jobs':8}\n",
    "    param['C'] = C\n",
    "    param['max_iter'] = int(max_iter)\n",
    "    MyModel = Model('lr', param)\n",
    "    val_mean, val_std = MyModel.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "    return val_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python -W ignore::DeprecationWarning\n",
    "''' A quick example: use bayesian optimization to tune the parameters '''\n",
    "opt = BayesianOptimization(score_lr, {'C':(0.25,5), 'max_iter':(50,500)})\n",
    "opt.maximize(n_iter=1)\n",
    "print(opt.res['max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### A Simple Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# A simple example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array([ (1, 1.2), (0.8, 2), (2, 2.7), (2.4, 2.9), (3, 2.5), (1.4, 0.6),\\\n",
    "                     (1.6, 2.1), (2.5, 2.1), (3.5, 1.2), (3.6,2.8)])\n",
    "y = np.array([-1,-1,-1,-1,-1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c218940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X[y==(-1),0], X[y==(-1),1], 'bo')\n",
    "plt.plot(X[y==1,0], X[y==1,1], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAAD8CAYAAADpLRYuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a459d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# need graphviz (conda install graphviz)\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "param = {'max_depth':1, 'learning_rate':1, 'reg_lambda':5}\n",
    "\n",
    "# train a gradient boosting tree\n",
    "bst = xgb.train(param, dtrain, num_boost_round=3)\n",
    "xgb.plot_tree(bst,num_trees=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Play with our train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847276001289 0.0406276456041\n"
     ]
    }
   ],
   "source": [
    "# Fit xgboost classifier\n",
    "MyData = Datawarehouse()\n",
    "MyData.read_data()\n",
    "\n",
    "param = {'silent': 1, 'seed':4242, 'objective':'binary:logistic', 'max_depth':6,\\\n",
    "        'learning_rate':0.05, 'nthread':8, 'reg_lambda':1, 'subsample': 0.7, \\\n",
    "        'colsample_bytree':0.5, 'colsample_bylevel':1, 'n_estimators':800, 'reg_alpha':0}\n",
    "\n",
    "MyModel = Model('xgb', param)\n",
    "val_mean, val_std = MyModel.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "print(val_mean, val_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MyModel.train(MyData.train_in, MyData.train_out)\n",
    "ypred = MyModel.predict(MyData.test_in)\n",
    "MyData.gen_submission(ypred, 'submission_xgb_all_features.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, mdl_type, param):\n",
    "        \"\"\" Initialize classififer based on classifier type and related parameters\n",
    "\n",
    "        Arguments:\n",
    "            mdl_type (string): a string to tell which classifier to use\n",
    "            param: a dictionary to store related parameters\n",
    "            \n",
    "        \"\"\"\n",
    "        self.param = param\n",
    "        self.mdl_type = mdl_type\n",
    "        if (mdl_type == 'xgb'):\n",
    "            self.clf = xgb.XGBClassifier(**param)\n",
    "        elif (mdl_type == 'lr'):\n",
    "            self.clf = LogisticRegression(**param)\n",
    "        elif (mdl_type == 'extra'):\n",
    "            self.clf = ExtraTreesClassifier(**param)\n",
    "        elif (mdl_type == 'rf'):\n",
    "            self.clf = RandomForestClassifier(**param)\n",
    "\n",
    "    # training the model based on input data\n",
    "    def train(self, data_in, data_out):\n",
    "        \"\"\" Train classifier based on input data and corresponding labels\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "        \"\"\"\n",
    "        if(self.mdl_type=='xgb'):\n",
    "            self.clf.fit(data_in, data_out, eval_metric='auc')\n",
    "        else:\n",
    "            self.clf.fit(data_in, data_out)\n",
    "\n",
    "    # split into k fold and find auc for each fold (for testing purpose)\n",
    "    def score_kfold(self, data_in, data_out, ns):\n",
    "        \"\"\" Get the score of k-fold cross validation\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "            num_fold: number of folds for cross validation \n",
    "        \n",
    "        Returns:\n",
    "            val_mean: mean of k-fold cross validation score\n",
    "            val_std: standard deviation of k-fold cross validation score\n",
    "        \"\"\"\n",
    "        # cut into k-folds\n",
    "        kf = KFold(n_splits=ns, shuffle=True)\n",
    "        self.score = []\n",
    "        for train_idx, test_idx in kf.split(data_in, y=data_out):\n",
    "            # divide into trainig and test set\n",
    "            train_in = data_in[train_idx, :]\n",
    "            train_out = data_out[train_idx]\n",
    "            test_in = data_in[test_idx, :]\n",
    "            test_out = data_out[test_idx]\n",
    "\n",
    "            # fit model\n",
    "            self.train(train_in, train_out)\n",
    "            test_pred = self.predict(test_in)\n",
    "\n",
    "            # get score\n",
    "            self.score.append(self.get_score(test_out, test_pred))\n",
    "\n",
    "        # return score mean and std\n",
    "        print(self.score)\n",
    "        return np.mean(self.score), np.std(self.score)\n",
    "\n",
    "    def score_group_kfold(self, data_in, data_out, groups, ns):\n",
    "        \"\"\" Get the score of group cross validation\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "            groups (ndarray): group information of input data\n",
    "            ns: number of splits for cross validation\n",
    "        \n",
    "        Returns:\n",
    "            val_mean: mean of k-fold cross validation score\n",
    "            val_std: standard deviation of k-fold cross validation score\n",
    "        \"\"\"\n",
    "        # cut into k-folds based on groups\n",
    "        gkf = GroupShuffleSplit(n_splits=ns, random_state=0)\n",
    "        self.score = []\n",
    "        for train_idx, test_idx in gkf.split(data_in, y=data_out, groups=groups):\n",
    "            # divide into trainig and test set\n",
    "            train_in = data_in[train_idx, :]\n",
    "            train_out = data_out[train_idx]\n",
    "            test_in = data_in[test_idx, :]\n",
    "            test_out = data_out[test_idx]\n",
    "\n",
    "            # fit model\n",
    "            self.train(train_in, train_out)\n",
    "            test_pred = self.predict(test_in)\n",
    "\n",
    "            # get score\n",
    "            self.score.append(self.get_score(test_out, test_pred))\n",
    "\n",
    "        return np.mean(self.score), np.std(self.score)\n",
    "\n",
    "    def gen_stacking(self, data_in, data_out, groups, data_test_in):\n",
    "        \"\"\" Generate probability vector for stacking\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix of train data\n",
    "            data_out (ndarray): binary output labels for each row of train data\n",
    "            groups (ndarray): group information of input data\n",
    "            data_test_in (ndarray): feature matrix of test data\n",
    "        \n",
    "        Returns:\n",
    "            ptrain (ndarray): probability prediction of train data\n",
    "            ptest (ndarray): probability prediction of test data\n",
    "        \"\"\"\n",
    "        kf = GroupKFold(n_splits=2)\n",
    "        ptrain = np.zeros(data_out.shape)\n",
    "        \n",
    "        for train_idx, test_idx in kf.split(data_in, y=data_out, groups=groups):\n",
    "            # divide into trainig and test set\n",
    "            train_in = data_in[train_idx, :]\n",
    "            train_out = data_out[train_idx]\n",
    "            test_in = data_in[test_idx, :]\n",
    "            test_out = data_out[test_idx]\n",
    "\n",
    "            # fit model\n",
    "            self.train(train_in, train_out)\n",
    "            ptrain[test_idx] = self.predict(test_in)\n",
    "        \n",
    "        self.train(data_in, data_out)\n",
    "        ptest = self.predict(data_test_in)\n",
    "        \n",
    "        return ptrain, ptest\n",
    "    \n",
    "    # prediction with probability outcome\n",
    "    def predict(self, data_in):\n",
    "        data_out = self.clf.predict_proba(data_in)[:, 1]\n",
    "        return data_out\n",
    "\n",
    "    # get the ROC curve\n",
    "    def get_score(self, test_out, test_pred):\n",
    "        fpr, tpr, _ = roc_curve(test_out, test_pred)\n",
    "        return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# after we finish the stacking function\n",
    "# read in data\n",
    "MyData = Datawarehouse()\n",
    "MyData.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit random forest classifier with different criteria\n",
    "param = {'n_estimators': 800,'criterion':'entropy','max_features':'sqrt','max_depth':10, \\\n",
    "        'min_samples_split':8, 'random_state':4242, 'n_jobs':8}\n",
    "MyModelRF = Model('rf', param)\n",
    "ptrainRF, ptestRF = MyModelRF.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n",
    "\n",
    "param['criterion'] = 'gini'\n",
    "MyModelRF = Model('rf', param)\n",
    "ptrainRF_2, ptestRF_2 = MyModelRF.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit xgboost classifier\n",
    "param = {'silent': 1, 'seed':4242, 'objective':'binary:logistic', 'max_depth':6,\\\n",
    "        'learning_rate':0.05, 'nthread':8, 'reg_lambda':1, 'subsample': 0.7, \\\n",
    "        'colsample_bytree':0.5, 'colsample_bylevel':1, 'n_estimators':800, 'reg_alpha':0}\n",
    "\n",
    "MyModelXGB = Model('xgb', param)\n",
    "ptrainXGB, ptestXGB = MyModelXGB.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit extra-tree classifier with different criteria\n",
    "param = {'n_estimators': 800,'criterion':'entropy','max_features':'sqrt','max_depth':10, \\\n",
    "        'min_samples_split':8, 'random_state':4242, 'n_jobs':8}\n",
    "MyModelEXTRA = Model('extra', param)\n",
    "ptrainEXTRA, ptestEXTRA = MyModelEXTRA.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n",
    "\n",
    "param['criterion'] = 'gini'\n",
    "MyModelEXTRA = Model('extra', param)\n",
    "ptrainEXTRA_2, ptestEXTRA_2 = MyModelEXTRA.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# combine probability predictions\n",
    "ptrain = np.vstack((ptrainRF, ptrainRF_2, ptrainXGB, ptrainEXTRA, ptrainEXTRA_2)).T\n",
    "ptest = np.vstack((ptestRF, ptestRF_2, ptestXGB, ptestEXTRA, ptestEXTRA_2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06819347,  0.06170278,  0.00340961,  0.08903363,  0.07514217],\n",
       "       [ 0.06924963,  0.06652953,  0.00294503,  0.07022511,  0.08219656],\n",
       "       [ 0.09778448,  0.14516028,  0.00250322,  0.11079706,  0.11902333],\n",
       "       ..., \n",
       "       [ 0.3711473 ,  0.32029298,  0.94588417,  0.26298824,  0.24659361],\n",
       "       [ 0.31346463,  0.27573516,  0.9412089 ,  0.23049187,  0.22838716],\n",
       "       [ 0.15748334,  0.06817169,  0.39872214,  0.19978248,  0.14990567]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Stacking\n",
    "param = {'max_iter':400, 'fit_intercept':True, 'n_jobs':8}\n",
    "MyWeight = Model('lr', param)\n",
    "\n",
    "MyWeight.train(ptrain, MyData.train_out)\n",
    "\n",
    "ypred = MyWeight.predict(ptest)\n",
    "MyData.gen_submission(ypred, 'submission_stacking.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
