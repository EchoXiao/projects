{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DS-207: Kaggle Competition | EEG Seizure Prediction Competition\n",
    "\n",
    ">Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective. Even after surgical removal of epilepsy, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring.\n",
    "\n",
    ">Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for electrical brain activity (EEG) based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.\\n\",\n",
    "\n",
    "From the competition [homepage](https://www.kaggle.com/c/melbourne-university-seizure-prediction).\n",
    "\n",
    "### Goal for this Notebook:\n",
    "* Use L1-regularized LR to select features\n",
    "* Parameter tuning using Bayesian optimization\n",
    "* Learn how to use xgboost\n",
    "* Use stacking to ensemble models\n",
    "\n",
    "#### Required Libraries:\n",
    "* [NumPy](http://www.numpy.org/)\n",
    "* [IPython](http://ipython.org/)\n",
    "* [Pandas](http://pandas.pydata.org/)\n",
    "* [SciKit-Learn](http://scikit-learn.org/stable/)\n",
    "* [Matplotlib](http://matplotlib.org/)\n",
    "* [Xgboost](http://xgboost.readthedocs.io/en/latest/build.html)\n",
    "* [bayes_opt]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Datawarehouse():\n",
    "    def __init__(self):\n",
    "        # Initialize Datawarehouse.\n",
    "        self.TRAIN_FILE = '../input/train.csv'\n",
    "        self.TEST_FILE = '../input/test.csv'\n",
    "        self.GROUP_FILE = '../input/group.csv'\n",
    "        self.DATA_ID = 'File'\n",
    "        self.DATA_OUT_NAME = 'Class'\n",
    "    \n",
    "    def read_data(self):\n",
    "        # Read in training and testing data for further processing\n",
    "        self.train_in = pd.read_csv(self.TRAIN_FILE)\n",
    "        self.test_in = pd.read_csv(self.TEST_FILE)\n",
    "        self.group = pd.read_csv(self.GROUP_FILE)\n",
    "        \n",
    "        # get response variable and id\n",
    "        self.col_name = self.train_in.columns\n",
    "        self.train_out = self.train_in[self.DATA_OUT_NAME]\n",
    "        self.train_id = self.train_in[self.DATA_ID]\n",
    "\n",
    "        # remove unnecessary information\n",
    "        del self.train_in[self.DATA_ID]\n",
    "        del self.train_in[self.DATA_OUT_NAME]\n",
    "\n",
    "        # get test id\n",
    "        self.test_id = self.test_in[self.DATA_ID]\n",
    "        \n",
    "        # remove unnecessary information from test as well\n",
    "        del self.test_in[self.DATA_ID]\n",
    "\n",
    "        # get group\n",
    "        del self.group[self.DATA_ID]\n",
    "\n",
    "        self.train_in = self.train_in.as_matrix()\n",
    "        self.train_out = self.train_out.as_matrix()\n",
    "        self.test_in = self.test_in.as_matrix()\n",
    "        self.group = self.group.as_matrix()\n",
    "    \n",
    "    def select_features(self, vec):\n",
    "        \"\"\" Select features given a boolean vector\n",
    "\n",
    "        Arguments:\n",
    "            vec (ndarray): a boolean vector to indicate what features to keep\n",
    "        \"\"\"\n",
    "        self.train_in = self.train_in[:,vec]\n",
    "        self.test_in = self.test_in[:,vec]\n",
    "    \n",
    "    def gen_submission(self, ypred, filename):\n",
    "        \"\"\" Generate submission files based on given prediction results\n",
    "\n",
    "        Arguments:\n",
    "            ypred (ndarray): a numeric vector storing probability\n",
    "        \"\"\"\n",
    "        dataset = list(zip(self.test_id, ypred))\n",
    "\n",
    "        # transform list to data frame\n",
    "        df_pred = pd.DataFrame(dataset, columns = [self.DATA_ID, self.DATA_OUT_NAME], index=None)\n",
    "        df_pred.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rand\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python -W ignore::DeprecationWarning\n",
    "from sklearn.model_selection import GroupShuffleSplit, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, mdl_type, param):\n",
    "        \"\"\" Initialize classififer based on classifier type and related parameters\n",
    "\n",
    "        Arguments:\n",
    "            mdl_type (string): a string to tell which classifier to use\n",
    "            param: a dictionary to store related parameters\n",
    "            \n",
    "        \"\"\"\n",
    "        self.param = param\n",
    "        self.mdl_type = mdl_type\n",
    "        if (mdl_type == 'xgb'):\n",
    "            self.clf = xgb.XGBClassifier(**param)\n",
    "        elif (mdl_type == 'lr'):\n",
    "            self.clf = LogisticRegression(**param)\n",
    "        elif (mdl_type == 'extra'):\n",
    "            self.clf = ExtraTreesClassifier(**param)\n",
    "        elif (mdl_type == 'rf'):\n",
    "            self.clf = RandomForestClassifier(**param)\n",
    "\n",
    "    # training the model based on input data\n",
    "    def train(self, data_in, data_out):\n",
    "        \"\"\" Train classifier based on input data and corresponding labels\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "        \"\"\"\n",
    "        if(self.mdl_type=='xgb'):\n",
    "            self.clf.fit(data_in, data_out, eval_metric='auc')\n",
    "        else:\n",
    "            self.clf.fit(data_in, data_out)\n",
    "\n",
    "    # split into k fold and find auc for each fold (for testing purpose)\n",
    "    def score_kfold(self, data_in, data_out, ns):\n",
    "        \"\"\" Get the score of k-fold cross validation\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "            num_fold: number of folds for cross validation \n",
    "        \n",
    "        Returns:\n",
    "            val_mean: mean of k-fold cross validation score\n",
    "            val_std: standard deviation of k-fold cross validation score\n",
    "        \"\"\"\n",
    "        # cut into k-folds\n",
    "        kf = KFold(n_splits=ns, shuffle=True)\n",
    "        self.score = []\n",
    "        for train_idx, test_idx in kf.split(data_in, y=data_out):\n",
    "            # divide into trainig and test set\n",
    "            train_in = data_in[train_idx, :]\n",
    "            train_out = data_out[train_idx]\n",
    "            test_in = data_in[test_idx, :]\n",
    "            test_out = data_out[test_idx]\n",
    "\n",
    "            # fit model\n",
    "            self.train(train_in, train_out)\n",
    "            test_pred = self.predict(test_in)\n",
    "\n",
    "            # get score\n",
    "            self.score.append(self.get_score(test_out, test_pred))\n",
    "\n",
    "        # return score mean and std\n",
    "        print(self.score)\n",
    "        return np.mean(self.score), np.std(self.score)\n",
    "\n",
    "    def score_group_kfold(self, data_in, data_out, groups, ns):\n",
    "        \"\"\" Get the score of group cross validation\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "            groups (ndarray): group information of input data\n",
    "            ns: number of splits for cross validation\n",
    "        \n",
    "        Returns:\n",
    "            val_mean: mean of k-fold cross validation score\n",
    "            val_std: standard deviation of k-fold cross validation score\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # prediction with probability outcome\n",
    "    def predict(self, data_in):\n",
    "        data_out = self.clf.predict_proba(data_in)[:, 1]\n",
    "        return data_out\n",
    "\n",
    "    # get the ROC curve\n",
    "    def get_score(self, test_out, test_pred):\n",
    "        fpr, tpr, _ = roc_curve(test_out, test_pred)\n",
    "        return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.790866416135 0.0463513338013\n"
     ]
    }
   ],
   "source": [
    "# use group to improve cross validation\n",
    "MyData = Datawarehouse()\n",
    "MyData.read_data()\n",
    "\n",
    "# setup model parameters\n",
    "param = {'C':1,'penalty':'l1','max_iter':400,'n_jobs':8}\n",
    "MyModelLR = Model('lr', param)\n",
    "\n",
    "# perform cross validation\n",
    "val_mean, val_std = MyModelLR.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "print(val_mean, val_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823487085659 0.0345062174434\n"
     ]
    }
   ],
   "source": [
    "# test random forest\n",
    "param = {'n_estimators': 800,'criterion':'entropy','max_features':'sqrt','max_depth':10, \\\n",
    "        'min_samples_split':8, 'random_state':4242, 'n_jobs':8}\n",
    "MyModelRF = Model('rf', param)\n",
    "\n",
    "val_mean, val_std = MyModelRF.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "print(val_mean, val_std)\n",
    "\n",
    "MyModelRF.train(MyData.train_in, MyData.train_out)\n",
    "ypred = MyModelRF.predict(MyData.test_in)\n",
    "MyData.gen_submission(ypred, 'submission_rf_all_features.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use l1-regularization to select features\n",
    "MyModelLR.train(MyData.train_in, MyData.train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1280389cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot the feature importance\n",
    "importances = MyModelLR.clf.coef_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(MyData.train_in.shape[1]), importances[0,indices[0]], color=\"r\",  align=\"center\")\n",
    "plt.xlim([-1, MyData.train_in.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.76843807e+00,  -3.61302229e+00,  -3.24169656e+00,\n",
       "        -2.71073605e+00,  -2.22588975e+00,  -2.22201614e+00,\n",
       "        -2.13047088e+00,  -2.02362998e+00,  -1.99828830e+00,\n",
       "        -1.87242695e+00,  -1.65236133e+00,  -1.61488738e+00,\n",
       "        -1.56647805e+00,  -1.44888153e+00,  -1.42205180e+00,\n",
       "        -1.41464870e+00,  -1.40185404e+00,  -1.39682450e+00,\n",
       "        -1.36206081e+00,  -1.35508065e+00,  -1.35018368e+00,\n",
       "        -1.34602988e+00,  -1.32924158e+00,  -1.30075737e+00,\n",
       "        -1.30046202e+00,  -1.28859638e+00,  -1.28423639e+00,\n",
       "        -1.27585471e+00,  -1.20729766e+00,  -1.19164621e+00,\n",
       "        -1.17547807e+00,  -1.17400481e+00,  -1.16720155e+00,\n",
       "        -1.15333575e+00,  -1.11223423e+00,  -1.08591953e+00,\n",
       "        -1.04270761e+00,  -1.02072680e+00,  -9.91390985e-01,\n",
       "        -9.56503004e-01,  -9.33680214e-01,  -9.17000160e-01,\n",
       "        -9.02856994e-01,  -8.95903555e-01,  -8.68245212e-01,\n",
       "        -8.04501652e-01,  -8.03895487e-01,  -7.95318573e-01,\n",
       "        -7.93006023e-01,  -7.67456109e-01,  -7.43113833e-01,\n",
       "        -7.34162303e-01,  -7.31733018e-01,  -7.09109220e-01,\n",
       "        -6.79538723e-01,  -6.75211433e-01,  -6.74099385e-01,\n",
       "        -6.72730731e-01,  -6.64515221e-01,  -6.56661309e-01,\n",
       "        -6.54783837e-01,  -6.54272723e-01,  -6.42075765e-01,\n",
       "        -6.37695013e-01,  -6.33425129e-01,  -6.16561819e-01,\n",
       "        -6.08178885e-01,  -6.07509390e-01,  -6.06173402e-01,\n",
       "        -5.97491375e-01,  -5.89963728e-01,  -5.89179245e-01,\n",
       "        -5.86517786e-01,  -5.84480632e-01,  -5.76521831e-01,\n",
       "        -5.74088779e-01,  -5.64120534e-01,  -5.56623330e-01,\n",
       "        -5.49306716e-01,  -5.37266404e-01,  -5.36293743e-01,\n",
       "        -5.23150365e-01,  -5.14815286e-01,  -5.08949147e-01,\n",
       "        -5.03309848e-01,  -4.87941514e-01,  -4.54294541e-01,\n",
       "        -4.52697771e-01,  -4.52246360e-01,  -4.34010106e-01,\n",
       "        -4.12893330e-01,  -4.11042759e-01,  -4.01635633e-01,\n",
       "        -4.01633609e-01,  -3.91542739e-01,  -3.85630209e-01,\n",
       "        -3.79040846e-01,  -3.77423571e-01,  -3.74044036e-01,\n",
       "        -3.72855976e-01,  -3.66522360e-01,  -3.63395426e-01,\n",
       "        -3.52461864e-01,  -3.37699657e-01,  -3.35033825e-01,\n",
       "        -3.32341374e-01,  -3.21645291e-01,  -3.19970055e-01,\n",
       "        -3.18445538e-01,  -3.11639812e-01,  -2.96322433e-01,\n",
       "        -2.90256987e-01,  -2.86632773e-01,  -2.86125071e-01,\n",
       "        -2.84798840e-01,  -2.82885524e-01,  -2.77905874e-01,\n",
       "        -2.71017376e-01,  -2.69406099e-01,  -2.64461118e-01,\n",
       "        -2.61531979e-01,  -2.59147897e-01,  -2.57004130e-01,\n",
       "        -2.55938284e-01,  -2.53183207e-01,  -2.47342013e-01,\n",
       "        -2.43327749e-01,  -2.37332456e-01,  -2.26621488e-01,\n",
       "        -2.24377687e-01,  -2.22970717e-01,  -2.19345344e-01,\n",
       "        -2.19031044e-01,  -2.06914501e-01,  -2.01800620e-01,\n",
       "        -1.93920161e-01,  -1.89828135e-01,  -1.88579675e-01,\n",
       "        -1.85480066e-01,  -1.78496428e-01,  -1.77549347e-01,\n",
       "        -1.76359121e-01,  -1.65048030e-01,  -1.63522722e-01,\n",
       "        -1.56341666e-01,  -1.51480551e-01,  -1.45976459e-01,\n",
       "        -1.43198408e-01,  -1.39071077e-01,  -1.37000181e-01,\n",
       "        -1.36779217e-01,  -1.30083353e-01,  -1.29818629e-01,\n",
       "        -1.29030198e-01,  -1.22264602e-01,  -1.20883795e-01,\n",
       "        -1.18364305e-01,  -1.15832040e-01,  -1.12728033e-01,\n",
       "        -1.08374990e-01,  -9.43348081e-02,  -9.07792445e-02,\n",
       "        -9.01942867e-02,  -8.72934280e-02,  -8.12962537e-02,\n",
       "        -7.89958152e-02,  -6.44412705e-02,  -5.41623264e-02,\n",
       "        -4.97062329e-02,  -4.51777857e-02,  -4.42721200e-02,\n",
       "        -4.23518768e-02,  -3.41066098e-02,  -3.06587307e-02,\n",
       "        -3.02314188e-02,  -2.25376515e-02,  -1.99796830e-02,\n",
       "        -1.28840172e-02,  -1.24645601e-02,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         9.00900752e-06,   8.05867403e-04,   9.18310067e-04,\n",
       "         8.64863924e-03,   8.88728015e-03,   1.10577803e-02,\n",
       "         1.54434052e-02,   1.84521232e-02,   1.94378609e-02,\n",
       "         2.61847343e-02,   3.20035794e-02,   3.23571464e-02,\n",
       "         3.29716787e-02,   4.06617357e-02,   4.06670441e-02,\n",
       "         4.16192746e-02,   4.45883166e-02,   4.47470458e-02,\n",
       "         4.98769722e-02,   6.30633248e-02,   6.36060289e-02,\n",
       "         6.48217265e-02,   6.62074262e-02,   6.74357233e-02,\n",
       "         7.10646856e-02,   7.23802874e-02,   7.49167194e-02,\n",
       "         7.98815240e-02,   8.27013453e-02,   8.84532795e-02,\n",
       "         9.18837538e-02,   9.37677186e-02,   9.51575028e-02,\n",
       "         9.85708274e-02,   9.87198999e-02,   9.98471655e-02,\n",
       "         1.13466873e-01,   1.14660741e-01,   1.18348677e-01,\n",
       "         1.20065448e-01,   1.22332787e-01,   1.23456641e-01,\n",
       "         1.35208775e-01,   1.41953095e-01,   1.47060169e-01,\n",
       "         1.47191341e-01,   1.48636611e-01,   1.57827343e-01,\n",
       "         1.71908912e-01,   1.76590077e-01,   1.77706011e-01,\n",
       "         1.78803058e-01,   1.78982954e-01,   1.86391542e-01,\n",
       "         1.87936010e-01,   2.09469592e-01,   2.12523274e-01,\n",
       "         2.17261599e-01,   2.19657120e-01,   2.20448195e-01,\n",
       "         2.21168747e-01,   2.25828696e-01,   2.28334690e-01,\n",
       "         2.32426984e-01,   2.37993003e-01,   2.39887430e-01,\n",
       "         2.43674054e-01,   2.49660381e-01,   2.58522016e-01,\n",
       "         2.63526094e-01,   2.70055554e-01,   2.94140613e-01,\n",
       "         3.01870392e-01,   3.04204335e-01,   3.06185580e-01,\n",
       "         3.15664224e-01,   3.18306804e-01,   3.20132512e-01,\n",
       "         3.22903418e-01,   3.32579817e-01,   3.35476014e-01,\n",
       "         3.41631541e-01,   3.44620345e-01,   3.53977946e-01,\n",
       "         3.54442730e-01,   3.56407290e-01,   3.62702123e-01,\n",
       "         3.66091786e-01,   3.67978110e-01,   3.73007763e-01,\n",
       "         3.74660038e-01,   3.75860679e-01,   3.87444689e-01,\n",
       "         3.90900603e-01,   3.93337498e-01,   3.99978435e-01,\n",
       "         4.07326467e-01,   4.31609403e-01,   4.35794860e-01,\n",
       "         4.40680199e-01,   4.43138723e-01,   4.44164558e-01,\n",
       "         4.63217128e-01,   4.76129506e-01,   4.78349939e-01,\n",
       "         4.87833588e-01,   5.03090332e-01,   5.04679179e-01,\n",
       "         5.16812320e-01,   5.17216038e-01,   5.20980908e-01,\n",
       "         5.37977627e-01,   5.57489651e-01,   5.63227312e-01,\n",
       "         5.66345536e-01,   5.72133668e-01,   5.78122491e-01,\n",
       "         6.02603751e-01,   6.02687774e-01,   6.08933203e-01,\n",
       "         6.23791993e-01,   6.34701231e-01,   6.51792925e-01,\n",
       "         6.69077093e-01,   6.71340380e-01,   6.87057808e-01,\n",
       "         6.90879351e-01,   7.40034218e-01,   7.40653145e-01,\n",
       "         7.42189881e-01,   7.50771345e-01,   7.51459294e-01,\n",
       "         7.54117433e-01,   7.61481326e-01,   7.81616477e-01,\n",
       "         8.02429070e-01,   8.03240821e-01,   8.15988743e-01,\n",
       "         8.20998557e-01,   8.29685077e-01,   8.47902815e-01,\n",
       "         8.63000526e-01,   8.66931650e-01,   8.72016565e-01,\n",
       "         8.84414958e-01,   8.94929944e-01,   8.97955701e-01,\n",
       "         9.01310635e-01,   9.02216909e-01,   9.09122161e-01,\n",
       "         9.15185958e-01,   9.35701423e-01,   9.37183128e-01,\n",
       "         9.69271125e-01,   9.69637529e-01,   9.89583999e-01,\n",
       "         1.01608259e+00,   1.02787384e+00,   1.04631361e+00,\n",
       "         1.04898542e+00,   1.05522408e+00,   1.06248175e+00,\n",
       "         1.06357790e+00,   1.06601651e+00,   1.07086334e+00,\n",
       "         1.09130325e+00,   1.09796865e+00,   1.10684023e+00,\n",
       "         1.17683143e+00,   1.17802310e+00,   1.18382697e+00,\n",
       "         1.20883872e+00,   1.21852003e+00,   1.21913426e+00,\n",
       "         1.26124038e+00,   1.27117601e+00,   1.30769197e+00,\n",
       "         1.31223685e+00,   1.31707222e+00,   1.41206446e+00,\n",
       "         1.76290341e+00,   1.77332575e+00,   1.80924416e+00,\n",
       "         1.84298227e+00,   1.84811322e+00,   1.88018744e+00,\n",
       "         1.90758354e+00,   2.17323808e+00,   2.32295753e+00,\n",
       "         2.51812475e+00,   2.71380613e+00,   2.88855844e+00,\n",
       "         2.93535351e+00,   3.28536599e+00])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances[0][indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TimeCorr_C6_C3', 'FreqCorr_C15_C15', 'TimeCorr_C14_C10',\n",
       "       'DyadicL12_C9', 'WaveletR5_C10', 'SigVar_C1', 'FreqCorr_C12_C6',\n",
       "       'TimeCorr_C8_C5', 'DyadicL8_C4', 'WaveletR8_C7', 'DyadicL5_C16',\n",
       "       'WaveletR2_C1', 'WaveletR7_C5', 'DyadicL10_C3', 'WaveletR1_C14',\n",
       "       'TimeCorr_C12_C8', 'DyadicL7_C13', 'WaveletR3_C12', 'FreqCorr_C10_C4',\n",
       "       'deltaPower_C5', 'WaveletR5_C16', 'WaveletR7_C7', 'WaveletR2_C8',\n",
       "       'spectralEdgeFreq_C15', 'highGammaPower_C16', 'DyadicL10_C10',\n",
       "       'Hurst_C10', 'TimeCorr_C15_C7', 'DyadicL4_C8', 'DyadicL7_C11',\n",
       "       'DyadicL3_C16', 'WaveletR4_C16', 'FreqCorr_C10_C2', 'TimeCorr_C6_C4',\n",
       "       'TimeEig_Rank1', 'TimeCorr_C12_C2', 'DyadicL5_C2', 'FreqCorr_C14_C6',\n",
       "       'FreqCorr_C16_C8', 'TimeEig_Rank4', 'DyadicL2_C6', 'TimeCorr_C15_C13',\n",
       "       'DyadicL7_C1', 'DyadicL9_C4', 'DyadicL2_C2', 'DyadicL1_C12',\n",
       "       'DyadicL11_C13', 'spectralEntropy_C4', 'DyadicL1_C11',\n",
       "       'lowGammaPower_C4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyData.col_name[indices][0,-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eliminate un-used features\n",
    "KeepVec = (importances!=0)\n",
    "MyData.select_features(KeepVec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4972, 373)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyData.train_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrain random forest classifier with reduced features\n",
    "param = {'n_estimators': 800,'criterion':'entropy','max_features':'sqrt','max_depth':10,\\\n",
    "        'min_samples_split':8, 'random_state':4242, 'n_jobs':8}\n",
    "MyModelRF = Model('rf', param)\n",
    "val_mean, val_std = MyModelRF.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "print(val_mean, val_std)\n",
    "\n",
    "# Train classifier using all the data\n",
    "MyModelRF.train(MyData.train_in, MyData.train_out)\n",
    "ypred = MyModelRF.predict(MyData.test_in)\n",
    "MyData.gen_submission(ypred, 'submission_rf_selected_features.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "* Use logistic regression as example\n",
    "* Play with Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "MyData = Datawarehouse()\n",
    "MyData.read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_lr(C, max_iter):\n",
    "    \"\"\" return score of an individual experiment\n",
    "    \"\"\"\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python -W ignore::DeprecationWarning\n",
    "''' A quick example: use bayesian optimization to tune the parameters '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array([ (1, 1.2), (0.8, 2), (2, 2.7), (2.4, 2.9), (3, 2.5), (1.4, 0.6),\\\n",
    "                     (1.6, 2.1), (2.5, 2.1), (3.5, 1.2), (3.6,2.8)])\n",
    "y = np.array([-1,-1,-1,-1,-1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X[y==(-1),0], X[y==(-1),1], 'bo')\n",
    "plt.plot(X[y==1,0], X[y==1,1], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need graphviz (conda install graphviz)\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "param = {'max_depth':1, 'learning_rate':1}\n",
    "\n",
    "# train a gradient boosting tree\n",
    "bst = xgb.train(param, dtrain, num_boost_round=3)\n",
    "xgb.plot_tree(bst,num_trees=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with our train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit xgboost classifier\n",
    "MyData = Datawarehouse()\n",
    "MyData.read_data()\n",
    "param = {'silent': 1, 'seed':4242, 'objective':'binary:logistic', 'max_depth':6,\\\n",
    "        'learning_rate':0.05, 'nthread':8, 'reg_lambda':1, 'subsample': 0.7, \\\n",
    "        'colsample_bytree':0.5, 'colsample_bylevel':1, 'n_estimators':800, 'reg_alpha':0}\n",
    "\n",
    "MyModel = Model('xgb', param)\n",
    "val_mean, val_std = MyModel.score_group_kfold(MyData.train_in, MyData.train_out, MyData.group, 6)\n",
    "print(val_mean, val_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MyModel.train(MyData.train_in, MyData.train_out)\n",
    "ypred = MyModel.predict(MyData.test_in)\n",
    "MyData.gen_submission(ypred, 'submission_xgb_all_features.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, mdl_type, param):\n",
    "        \"\"\" Initialize classififer based on classifier type and related parameters\n",
    "\n",
    "        Arguments:\n",
    "            mdl_type (string): a string to tell which classifier to use\n",
    "            param: a dictionary to store related parameters\n",
    "            \n",
    "        \"\"\"\n",
    "        self.param = param\n",
    "        self.mdl_type = mdl_type\n",
    "        if (mdl_type == 'xgb'):\n",
    "            self.clf = xgb.XGBClassifier(**param)\n",
    "        elif (mdl_type == 'lr'):\n",
    "            self.clf = LogisticRegression(**param)\n",
    "        elif (mdl_type == 'extra'):\n",
    "            self.clf = ExtraTreesClassifier(**param)\n",
    "        elif (mdl_type == 'rf'):\n",
    "            self.clf = RandomForestClassifier(**param)\n",
    "\n",
    "    # training the model based on input data\n",
    "    def train(self, data_in, data_out):\n",
    "        \"\"\" Train classifier based on input data and corresponding labels\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "        \"\"\"\n",
    "        if(self.mdl_type=='xgb'):\n",
    "            self.clf.fit(data_in, data_out, eval_metric='auc')\n",
    "        else:\n",
    "            self.clf.fit(data_in, data_out)\n",
    "\n",
    "    # split into k fold and find auc for each fold (for testing purpose)\n",
    "    def score_kfold(self, data_in, data_out, ns):\n",
    "        \"\"\" Get the score of k-fold cross validation\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "            num_fold: number of folds for cross validation \n",
    "        \n",
    "        Returns:\n",
    "            val_mean: mean of k-fold cross validation score\n",
    "            val_std: standard deviation of k-fold cross validation score\n",
    "        \"\"\"\n",
    "        # cut into k-folds\n",
    "        kf = KFold(n_splits=ns, shuffle=True)\n",
    "        self.score = []\n",
    "        for train_idx, test_idx in kf.split(data_in, y=data_out):\n",
    "            # divide into trainig and test set\n",
    "            train_in = data_in[train_idx, :]\n",
    "            train_out = data_out[train_idx]\n",
    "            test_in = data_in[test_idx, :]\n",
    "            test_out = data_out[test_idx]\n",
    "\n",
    "            # fit model\n",
    "            self.train(train_in, train_out)\n",
    "            test_pred = self.predict(test_in)\n",
    "\n",
    "            # get score\n",
    "            self.score.append(self.get_score(test_out, test_pred))\n",
    "\n",
    "        # return score mean and std\n",
    "        print(self.score)\n",
    "        return np.mean(self.score), np.std(self.score)\n",
    "\n",
    "    def score_group_kfold(self, data_in, data_out, groups, ns):\n",
    "        \"\"\" Get the score of group cross validation\n",
    "\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix for training classifier\n",
    "            data_out (ndarray): binary output labels for each row of feature matrix\n",
    "            groups (ndarray): group information of input data\n",
    "            ns: number of splits for cross validation\n",
    "        \n",
    "        Returns:\n",
    "            val_mean: mean of k-fold cross validation score\n",
    "            val_std: standard deviation of k-fold cross validation score\n",
    "        \"\"\"\n",
    "        # cut into k-folds based on groups\n",
    "        gkf = GroupShuffleSplit(n_splits=ns, random_state=0)\n",
    "        self.score = []\n",
    "        for train_idx, test_idx in gkf.split(data_in, y=data_out, groups=groups):\n",
    "            # divide into trainig and test set\n",
    "            train_in = data_in[train_idx, :]\n",
    "            train_out = data_out[train_idx]\n",
    "            test_in = data_in[test_idx, :]\n",
    "            test_out = data_out[test_idx]\n",
    "\n",
    "            # fit model\n",
    "            self.train(train_in, train_out)\n",
    "            test_pred = self.predict(test_in)\n",
    "\n",
    "            # get score\n",
    "            self.score.append(self.get_score(test_out, test_pred))\n",
    "\n",
    "        return np.mean(self.score), np.std(self.score)\n",
    "\n",
    "    def gen_stacking(self, data_in, data_out, groups, data_test_in):\n",
    "        \"\"\" Generate probability vector for stacking\n",
    "        Arguments:\n",
    "            data_in (ndarray): feature matrix of train data\n",
    "            data_out (ndarray): binary output labels for each row of train data\n",
    "            groups (ndarray): group information of input data\n",
    "            data_test_in (ndarray): feature matrix of test data\n",
    "        \n",
    "        Returns:\n",
    "            ptrain (ndarray): probability prediction of train data\n",
    "            ptest (ndarray): probability prediction of test data\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    # prediction with probability outcome\n",
    "    def predict(self, data_in):\n",
    "        data_out = self.clf.predict_proba(data_in)[:, 1]\n",
    "        return data_out\n",
    "\n",
    "    # get the ROC curve\n",
    "    def get_score(self, test_out, test_pred):\n",
    "        fpr, tpr, _ = roc_curve(test_out, test_pred)\n",
    "        return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# after we finish the stacking function\n",
    "# read in data\n",
    "MyData = Datawarehouse()\n",
    "MyData.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit random forest classifier with different criteria\n",
    "param = {'n_estimators': 800,'criterion':'entropy','max_features':'sqrt','max_depth':10, \\\n",
    "        'min_samples_split':8, 'random_state':4242, 'n_jobs':8}\n",
    "MyModelRF = Model('rf', param)\n",
    "ptrainRF, ptestRF = MyModelRF.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n",
    "\n",
    "param['criterion'] = 'gini'\n",
    "MyModelRF = Model('rf', param)\n",
    "ptrainRF_2, ptestRF_2 = MyModelRF.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit xgboost classifier\n",
    "param = {'silent': 1, 'seed':4242, 'objective':'binary:logistic', 'max_depth':6,\\\n",
    "        'learning_rate':0.05, 'nthread':8, 'reg_lambda':1, 'subsample': 0.7, \\\n",
    "        'colsample_bytree':0.5, 'colsample_bylevel':1, 'n_estimators':800, 'reg_alpha':0}\n",
    "\n",
    "MyModelXGB = Model('xgb', param)\n",
    "ptrainXGB, ptestXGB = MyModelXGB.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit extra-tree classifier with different criteria\n",
    "param = {'n_estimators': 800,'criterion':'entropy','max_features':'sqrt','max_depth':10, \\\n",
    "        'min_samples_split':8, 'random_state':4242, 'n_jobs':8}\n",
    "MyModelEXTRA = Model('extra', param)\n",
    "ptrainEXTRA, ptestEXTRA = MyModelEXTRA.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n",
    "\n",
    "param['criterion'] = 'gini'\n",
    "MyModelEXTRA = Model('extra', param)\n",
    "ptrainEXTRA_2, ptestEXTRA_2 = MyModelEXTRA.gen_stacking(MyData.train_in, MyData.train_out, MyData.group, MyData.test_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine probability predictions\n",
    "ptrain = np.vstack((ptrainRF, ptrainRF_2, ptrainXGB, ptrainEXTRA, ptrainEXTRA_2)).T\n",
    "ptest = np.vstack((ptestRF, ptestRF_2, ptestXGB, ptestEXTRA, ptestEXTRA_2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Stacking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
